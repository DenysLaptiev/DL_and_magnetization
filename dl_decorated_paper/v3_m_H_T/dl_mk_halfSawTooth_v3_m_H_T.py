import logging
import numpy as np
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torch.optim.lr_scheduler import StepLR

# –∏—Å–ø–æ–ª—å–∑—É–µ–º torch
# 1. –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –≥—Ä–∞—Ñ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
# PyTorch —Å—Ç—Ä–æ–∏—Ç –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–π –≥—Ä–∞—Ñ ¬´–Ω–∞ —Ö–æ–¥—É¬ª (eager execution).
#
# 2. –ü—Ä–æ—Å—Ç–æ—Ç–∞ –∏ ¬´–ø–∏—Ç–æ–Ω–∏—á–Ω–æ—Å—Ç—å¬ª
# API PyTorch –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –±–ª–∏–∑–æ–∫ –∫ —á–∏—Å—Ç–æ–º—É Python/Numpy-–∫–æ–¥—É.
#
# 3. –ë–æ–ª—å—à–æ–µ —Å–æ–æ–±—â–µ—Å—Ç–≤–æ –∏ —ç–∫–æ—Å–∏—Å—Ç–µ–º–∞
# PyTorch –Ω–∞ —Å–µ–≥–æ–¥–Ω—è ‚Äî –æ–¥–∏–Ω –∏–∑ —Å–∞–º—ã—Ö –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–æ–≤ –≤ –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–æ–π —Å—Ä–µ–¥–µ (–±–æ–ª–µ–µ 60 % –ø—É–±–ª–∏–∫–∞—Ü–∏–π ML/—Å—Ç–∞—Ç–µ–π –∏—Å–ø–æ–ª—å–∑—É—é—Ç –µ–≥–æ –¥–ª—è —Ä–µ–ø—Ä–æducibility), –∞ —Ç–∞–∫–∂–µ –∞–∫—Ç–∏–≤–Ω–æ –Ω–∞–±–∏—Ä–∞–µ—Ç –¥–æ–ª—é –≤ –∏–Ω–¥—É—Å—Ç—Ä–∏–∏.

from dl_decorated_paper import math_utils

# ============================
# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
# ============================

#--------------File names
TRAINED_MODEL_FILE = 'model_decorated_mHT.pth'
LEARNING_CURVE_PLOT_FILE = 'learning_curve_decorated_mHT.png'

#--------------PhysicalSystem parameters
COMPUTE_TYPE = 'analytical'
LATTICE_TYPE = 'decorated'

H_MIN = 0
H_MAX = 10
H_POINTS_NUMBER = 64
H_values = np.linspace(H_MIN, H_MAX, H_POINTS_NUMBER)

#T = 1.0
T_MIN, T_MAX, T_POINTS_NUMBER = 0.1, 1.0, 8
T_values = np.linspace(T_MIN, T_MAX, T_POINTS_NUMBER)

J_MIN = -1
J_MAX = +1

# connection between code parameters and paper parameters
# J1=Jd
# J2=Jd
# J3=J
# J4=Jt

# lattice types
# 'general' -> J1, J2, J3, J4
# 'decorated' -> J1, J2=J1, J3, J4
# 'molecule' -> J1, J2=J1, J3=J1, J4=0
# 'simple' -> J1, J2=J1, J3=0, J4=J1

#--------------Training parameters
#–°–∫–æ–ª—å–∫–æ —Å–ª—É—á–∞–π–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ (–ø–∞—Ä ¬´–ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç—å m(H,T) ‚Üí J-–ø–∞—Ä–∞–º–µ—Ç—Ä—ã¬ª) –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç—Å—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.
TRAIN_SAMPLES_NUMBER = 50000

#–†–∞–∑–º–µ—Ä –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–∞, –Ω–∞ –∫–æ—Ç–æ—Ä–æ–º –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ—Ç—Å—è ¬´–æ–±—â–∞—è¬ª –æ—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ (–Ω–µ —É—á–∞—Å—Ç–≤—É–µ—Ç –≤ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–º —Å–ø—É—Å–∫–µ).
VAL_SAMPLES_NUMBER = 5000

#–†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–∞, –∫–æ—Ç–æ—Ä—ã–º –º—ã –æ—Ü–µ–Ω–∏–≤–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –ø–æ—Å–ª–µ –æ–∫–æ–Ω—á–∞–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è.
TEST_SAMPLES_NUMBER = 5000

#–°–∫–æ–ª—å–∫–æ —Ü–µ–ª—ã—Ö –ø—Ä–æ—Ö–æ–¥–æ–≤ –ø–æ –≤—Å–µ–º—É —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–º—É –Ω–∞–±–æ—Ä—É –¥–∞–Ω–Ω—ã—Ö –±—É–¥–µ—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–æ.
num_epochs = 300

#–†–∞–∑–º–µ—Ä –º–∏–Ω–∏-–±–∞—Ç—á–∞ ‚Äî —Å–∫–æ–ª—å–∫–æ –æ–±—Ä–∞–∑—Ü–æ–≤ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –ø—Ä–æ–≥–æ–Ω—è–µ—Ç—Å—è —á–µ—Ä–µ–∑ —Å–µ—Ç—å –ø–µ—Ä–µ–¥ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ–º –≤–µ—Å–æ–≤.
BATCH_SIZE_PARAMETER = 64

#–ù–∞—á–∞–ª—å–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è (—à–∞–≥ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞) –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ Adam.
LEARNING_RATE_PARAMETER = 0.001
'''
                    –ü—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è
1. –£ –Ω–∞—Å –µ—Å—Ç—å TRAIN_SAMPLES_NUMBER –æ–±—Ä–∞–∑—Ü–æ–≤.
2. –ú—ã —Ä–∞–∑–±–∏–≤–∞–µ–º –∏—Ö –Ω–∞ –º–∏–Ω–∏-–±–∞—Ç—á–∏ –ø–æ BATCH_SIZE_PARAMETER —à—Ç—É–∫.
3. –î–ª—è –∫–∞–∂–¥–æ–≥–æ –±–∞—Ç—á–∞ –º—ã:

–¥–µ–ª–∞–µ–º forward (–ø—Ä–æ–≥–æ–Ω —á–µ—Ä–µ–∑ —Å–µ—Ç—å),
–≤—ã—á–∏—Å–ª—è–µ–º loss,
backward (–≥—Ä–∞–¥–∏–µ–Ω—Ç—ã) –∏
optimizer.step() (–æ–±–Ω–æ–≤–ª—è–µ–º –≤–µ—Å–∞).

4. –ö–æ–≥–¥–∞ –º—ã –ø–µ—Ä–µ–±—Ä–∞–ª–∏ –≤—Å–µ –±–∞—Ç—á–∏ (—Ç–æ –µ—Å—Ç—å –ø—Ä–æ—à–ª–∏ –ø–æ –≤—Å–µ–º TRAIN_SAMPLES_NUMBER –æ–±—Ä–∞–∑—Ü–∞–º),
—ç—Ç–æ –∑–∞–≤–µ—Ä—à–∞–µ—Ç –æ–¥–Ω—É —ç–ø–æ—Ö—É.
5. –ó–∞—Ç–µ–º –º—ã —Å–Ω–æ–≤–∞ –Ω–∞—á–∏–Ω–∞–µ–º –ø–µ—Ä–µ–±–æ—Ä –±–∞—Ç—á–µ–π —Å –ø–µ—Ä–≤–æ–≥–æ, –∏ —Ç–∞–∫ –ø–æ–≤—Ç–æ—Ä—è–µ–º num_epochs —Ä–∞–∑.

–ï—Å–ª–∏ –ø–æ—Å–ª–µ–¥–Ω–∏–π –±–∞—Ç—á —Ä–∞–∑–º–µ—Ä–æ–º –º–µ–Ω—å—à–µ BATCH_SIZE_PARAMETER, –æ–Ω —Ç–æ–∂–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è —Ç–æ—á–Ω–æ —Ç–∞–∫ –∂–µ.


–í –¥–∞–Ω–Ω–æ–º –∫–æ–¥–µ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ train_loader –º—ã –ø–µ—Ä–µ–¥–∞—ë–º shuffle=True:

train_loader = DataLoader(train_dataset,
                          batch_size=BATCH_SIZE_PARAMETER,
                          shuffle=True)
–≠—Ç–æ –∑–Ω–∞—á–∏—Ç, —á—Ç–æ –ø—Ä–∏ –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–µ PyTorch –±—É–¥–µ—Ç –ø–µ—Ä–µ–º–µ—à–∏–≤–∞—Ç—å –≤–µ—Å—å —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –ø–µ—Ä–µ–¥ —Ç–µ–º, –∫–∞–∫ —Ä–∞–∑–±–∏—Ç—å –µ–≥–æ –Ω–∞ –±–∞—Ç—á–∏. –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ:

–ë–∞—Ç—á–∏ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —ç–ø–æ—Ö–∞—Ö –±—É–¥—É—Ç —Å–æ—Å—Ç–æ—è—Ç—å –∏–∑ —Å–ª—É—á–∞–π–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –æ–±—Ä–∞–∑—Ü–æ–≤.
–í –æ–¥–Ω–æ–º –∏ —Ç–æ–º –∂–µ –±–∞—Ç—á–µ –≤ –¥–≤—É—Ö —Ä–∞–∑–Ω—ã—Ö —ç–ø–æ—Ö–∞—Ö –º–æ–≥—É—Ç –æ–∫–∞–∑–∞—Ç—å—Å—è —Ä–∞–∑–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã.

–î–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∏ —Ç–µ—Å—Ç–∞ –º—ã –æ–±—ã—á–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º shuffle=False, —á—Ç–æ–±—ã –æ—Ü–µ–Ω–∫–∏ –±—ã–ª–∏ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º—ã –∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã:

val_loader   = DataLoader(val_dataset, batch_size=‚Ä¶, shuffle=False)
test_loader  = DataLoader(test_dataset, batch_size=‚Ä¶, shuffle=False)
'''

# TRAIN_SAMPLES_NUMBER = 1000  # —É–≤–µ–ª–∏—á–∏–ª–∏
# VAL_SAMPLES_NUMBER = 200
# TEST_SAMPLES_NUMBER = 200
# num_epochs = 5  # –±–æ–ª—å—à–µ —ç–ø–æ—Ö
# BATCH_SIZE_PARAMETER = 32  # –±–æ–ª—å—à–µ –±–∞—Ç—á–µ–π
# LEARNING_RATE_PARAMETER = 0.001

#--------------Monte-Carlo parameters (for generation of Dataset)
#–í —Ç–µ–∫—É—â–µ–º —Å–∫—Ä–∏–ø—Ç–µ —ç—Ç–∏ 3 –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è, –Ω–æ –ø—Ä–∏–≥–æ–¥—è—Ç—Å—è, –µ—Å–ª–∏ –±—É–¥–µ–º –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∫—Ä–∏–≤—ã–µ/–ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–∏ –º–µ—Ç–æ–¥–æ–º –ú–ö –≤–º–µ—Å—Ç–æ –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–æ–π —Ñ—É–Ω–∫—Ü–∏–∏.
#—á–∏—Å–ª–æ —Å–ø–∏–Ω–æ–≤ (—è—á–µ–µ–µ–∫) –≤ —Ü–µ–ø–æ—á–∫–µ
num_cells = 20

#–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ ¬´—Ä–∞—Å–∫—Ä—É—Ç–∫–∏¬ª —Å–∏—Å—Ç–µ–º—ã –¥–æ —Ä–∞–≤–Ω–æ–≤–µ—Å–∏—è
num_steps = 10000

#—á–∏—Å–ª–æ —à–∞–≥–æ–≤ –∏–∑–º–µ—Ä–µ–Ω–∏—è –ø–æ—Å–ª–µ —ç–∫–≤–∏–ª–∏–±—Ä–æ–≤–∫–∏
equil_steps = 500

'''
                    Monte-Carlo –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞–ø–∞ 
                    (–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞–ø—É –∏–∑ —Ç–æ—á–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–æ–π —Ñ–æ—Ä–º—É–ª—ã)

–ü–æ—è—Å–Ω–∏–º –∑–Ω–∞—á–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.
1) equil_steps (—ç–∫–≤–∏–ª–∏–±—Ä–æ–≤–∫–∞, warm-up)

–ü—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —á–∏—Å–ª–æ MC-—à–∞–≥–æ–≤ (–∏—Ç–µ—Ä–∞—Ü–∏–π), –∫–æ—Ç–æ—Ä—ã–µ –º—ã –≤—ã–ø–æ–ª–Ω—è–µ–º –¥–æ —Ç–æ–≥–æ, 
–∫–∞–∫ –Ω–∞—á–∏–Ω–∞—Ç—å —Å–æ–±–∏—Ä–∞—Ç—å –∏–∑–º–µ—Ä–µ–Ω–∏—è.

–¶–µ–ª—å ‚Äî ¬´—Ä–∞—Å–∫—Ä—É—Ç–∏—Ç—å¬ª —Å–∏—Å—Ç–µ–º—É –∏–∑ —Å–ª—É—á–∞–π–Ω–æ–≥–æ –Ω–∞—á–∞–ª—å–Ω–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è, 
—á—Ç–æ–±—ã –æ–Ω–∞ –ø—Ä–∏—à–ª–∞ –≤ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞–≤–Ω–æ–≤–µ—Å–∏–µ –ø—Ä–∏ –∑–∞–¥–∞–Ω–Ω—ã—Ö ùêª,ùëá,ùêΩ.

–ë–µ–∑ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–π —ç–∫–≤–∏–ª–∏–±—Ä–æ–≤–∫–∏ –≤—ã —Ä–∏—Å–∫—É–µ—Ç–µ –∏–∑–º–µ—Ä—è—Ç—å –º–æ–º–µ–Ω—Ç–∞–ª—å–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ, 
–µ—â—ë –Ω–µ ¬´—É—Å—Ç–æ—è–≤—à–∏–µ—Å—è¬ª –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏, —á—Ç–æ –¥–∞—Å—Ç –∏—Å–∫–∞–∂—ë–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã.

2) num_steps (—Å–±–æ—Ä–∫–∞ –∏–∑–º–µ—Ä–µ–Ω–∏–π, sampling)

–≠—Ç–æ —á–∏—Å–ª–æ MC-—à–∞–≥–æ–≤ –ø–æ—Å–ª–µ —ç–∫–≤–∏–ª–∏–±—Ä–æ–≤–∫–∏, –≤ —Ö–æ–¥–µ –∫–æ—Ç–æ—Ä—ã—Ö –º—ã —Ñ–∏–∫c–∏—Ä—É–µ–º —Ç–µ–∫—É—â—É—é –Ω–∞–º–∞–≥–Ω–∏—á–µ–Ω–Ω–æ—Å—Ç—å 
(–∏/–∏–ª–∏ –¥—Ä—É–≥–∏–µ –Ω–∞–±–ª—é–¥–∞–µ–º—ã–µ) –¥–ª—è —É—Å—Ä–µ–¥–Ω–µ–Ω–∏—è.

–ß–µ–º –±–æ–ª—å—à–µ num_steps, —Ç–µ–º —Ç–æ—á–Ω–µ–µ –æ—Ü–µ–Ω–∫–∞ —Å—Ä–µ–¥–Ω–µ–≥–æ ùëö(ùêª,ùëá), 
–ø–æ—Ç–æ–º—É —á—Ç–æ —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ –∏–¥—ë—Ç –ø–æ –±–æ–ª—å—à–µ–º—É —á–∏—Å–ª—É —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π.

–û–±—ã—á–Ω–æ –º–µ–∂–¥—É –∏–∑–º–µ—Ä–µ–Ω–∏—è–º–∏ –¥–µ–ª–∞—é—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö MC-—à–∞–≥–æ–≤, 
—á—Ç–æ–±—ã —Å–Ω–∏–∑–∏—Ç—å –∞–≤—Ç–æ–∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é, –Ω–æ –≤ –ø—Ä–æ—Å—Ç–æ–º –≤–∞—Ä–∏–∞–Ω—Ç–µ –º–æ–∂–Ω–æ –∏–∑–º–µ—Ä—è—Ç—å –∫–∞–∂–¥—ã–π —à–∞–≥.
'''



# ============================
# –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
# ============================

def normalize(X):
    # X.shape = (N, H) ‚Äî N (–Ω–∞–ø—Ä–∏–º–µ—Ä N=TRAIN_SAMPLES_NUMBER) –æ–±—Ä–∞–∑—Ü–æ–≤, –∫–∞–∂–¥—ã–π –¥–ª–∏–Ω—ã H_POINTS_NUMBER (—á–∏—Å–ª–æ —Ç–æ—á–µ–∫ –ø–æ H_values)

    # —Å—á–∏—Ç–∞–µ–º —Å—Ä–µ–¥–Ω–µ–µ –¥–ª—è –∫–∞–∂–¥–æ–π –∫—Ä–∏–≤–æ–π
    mean = np.mean(X, axis=1, keepdims=True)
    # mean.shape = (N, 1)

    # —Å—á–∏—Ç–∞–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –¥–ª—è –∫–∞–∂–¥–æ–π –∫—Ä–∏–≤–æ–π
    std = np.std(X, axis=1, keepdims=True) + 1e-8
    # std.shape = (N, 1); +1e-8 ‚Äî —á—Ç–æ–±—ã –Ω–µ –¥–µ–ª–∏—Ç—å –Ω–∞ –Ω–æ–ª—å

    # —Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º –∫–∞–∂–¥–∞—è –∫—Ä–∏–≤–∞—è –ø–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –∏–º–µ–µ—Ç —Å—Ä–µ–¥–Ω–µ–µ 0 –∏ –¥–∏—Å–ø–µ—Ä—Å–∏—é 1
    return (X - mean) / std

def normalize_2d(X):
    # X.shape = (N,T, H)
    # N ‚Äî —á–∏—Å–ª–æ –æ–±—Ä–∞–∑—Ü–æ–≤ (TRAIN_SAMPLES_NUMBER)
    # T ‚Äî —á–∏—Å–ª–æ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–Ω—ã—Ö —Ç–æ—á–µ–∫ (T_POINTS_NUMBER)
    # H ‚Äî —á–∏—Å–ª–æ –ø–æ–ª–µ–≤—ã—Ö —Ç–æ—á–µ–∫ (H_POINTS_NUMBER)

    # –°—á–∏—Ç–∞–µ–º –æ–±—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ –ø–æ –∫–∞–∂–¥–æ–π –∫–∞—Ä—Ç–µ:
    # —É—Å—Ä–µ–¥–Ω—è–µ–º —Å–Ω–∞—á–∞–ª–∞ –ø–æ –æ—Å—è–º T –∏ H, –æ—Å—Ç–∞—ë—Ç—Å—è –æ–¥–∏–Ω —Å—Ä–µ–¥–Ω–∏–π –Ω–∞ –∫–∞–∂–¥—ã–π –∏–∑ N –æ–±—Ä–∞–∑—Ü–æ–≤
    mean = X.mean(axis=(1,2), keepdims=True)
    # mean.shape == (N, 1, 1)

    # —Å—á–∏—Ç–∞–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –¥–ª—è –∫–∞–∂–¥–æ–π –∫—Ä–∏–≤–æ–π
    std  = X.std (axis=(1,2), keepdims=True) + 1e-8
    # std.shape == (N, 1, 1), +1e-8 —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –¥–µ–ª–µ–Ω–∏—è –Ω–∞ 0

    # –í—ã—á–∏—Ç–∞–µ–º —Å—Ä–µ–¥–Ω–µ–µ –∏ –¥–µ–ª–∏–º –Ω–∞ std –¥–ª—è –∫–∞–∂–¥–æ–π —Ç–æ—á–∫–∏:
    # –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –∫–∞–∂–¥–∞—è –∫–∞—Ä—Ç–∞ –∏–º–µ–µ—Ç —Å—Ä–µ–¥–Ω–µ–µ 0 –∏ –¥–∏—Å–ø–µ—Ä—Å–∏—é 1
    return (X - mean) / std


# ============================
# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
# ============================

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–ª—É—á–∞–π–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ J –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ –æ—Ç J_MIN –¥–æ J_MAX
def random_J(lattice_type):
    if lattice_type == 'general':
        J1, J2, J3, J4 = np.random.uniform(J_MIN, J_MAX, 4)
        return J1, J2, J3, J4
    elif lattice_type == 'decorated':
        J1, J2, J3, J4 = np.random.uniform(J_MIN, J_MAX, 4)
        J2 = J1
        return J1, J2, J3, J4
    elif lattice_type == 'molecule':
        J1, J2, J3, J4 = np.random.uniform(J_MIN, J_MAX, 4)
        J2 = J1
        J3 = J1
        J4 = 0
        return J1, J2, J3, J4
    elif lattice_type == 'simple':
        J1, J2, J3, J4 = np.random.uniform(J_MIN, J_MAX, 4)
        J2 = J1
        J3 = 0
        J4 = J1
        return J1, J2, J3, J4
    else:
        J1, J2, J3, J4 =0,0,0,0
        return J1, J2, J3, J4

# m(H) for fixed T = 1.0
# def generate_sample(J_params):
#     J1, J2, J3, J4 = J_params
#     Jd = J1
#     J = J3
#     Jt = J4
#     mag_curve = [math_utils.m(J, Jd, Jt, H, T) for H in H_values]
#     # –ù–∞ –≤—ã—Ö–æ–¥–µ –∏–º–µ–µ–º 1D –º–∞—Å—Å–∏–≤ –¥–ª–∏–Ω—ã H_POINTS_NUMBER - —Ç–æ—á–∫–∏ –Ω–∞ –∫—Ä–∏–≤–æ–π –Ω–∞–º–∞–≥–Ω–∏—á–µ–Ω–Ω–æ—Å—Ç–∏.
#     # –≠–ª–µ–º–µ–Ω—Ç—ã –º–∞—Å—Å–∏–≤–∞ - –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞–º–∞–≥–Ω–∏—á–µ–Ω–Ω–æ—Å—Ç–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –ø–æ–ª—è–º –∏–∑ H_POINTS_NUMBER –ø—Ä–∏ –¥–∞–Ω–Ω—ã—Ö J,Jd,Jt –∏ T
#     return np.array(mag_curve)



# m(H) for T_values (0.1,1.0)
def generate_sample_2d(J_params):
    J1, J2, J3, J4 = J_params
    Jd, J, Jt = J1, J3, J4

    # –°–æ–∑–¥–∞—ë–º ¬´–ø—É—Å—Ç—É—é¬ª –∫–∞—Ä—Ç—É –Ω–∞–º–∞–≥–Ω–∏—á–µ–Ω–Ω–æ—Å—Ç–∏:
    # mag_map.shape = (T_POINTS_NUMBER, H_POINTS_NUMBER)
    mag_map = np.zeros((len(T_values), len(H_values)), dtype=np.float32)

    # –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ –∫–∞–∂–¥–æ–π —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–µ
    for ti, T in enumerate(T_values):
        # –î–ª—è —Ç–µ–∫—É—â–µ–≥–æ T —Å—á–∏—Ç–∞–µ–º 1D-–∫—Ä–∏–≤—É—é m(H) —Ç–æ—á–Ω–æ —Ç–∞–∫ –∂–µ,
        # –∫–∞–∫ –≤ generate_sample, –∏ –∑–∞–ø–∏—Å—ã–≤–∞–µ–º –≤ —Å—Ç—Ä–æ–∫—É mag_map[ti]
        mag_map[ti] = [math_utils.m(J, Jd, Jt, H, T) for H in H_values]

    # –ù–∞ –≤—ã—Ö–æ–¥–µ –¥–∞—ë–º –¥–≤—É–º–µ—Ä–Ω—É—é –∫–∞—Ä—Ç—É:
    #   mag_map[ti, hi] = m(H_values[hi], T_values[ti]; J, Jd, Jt)
    return mag_map



'''
                            MagnetizationDataset
–≠—Ç–æ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –∫–ª–∞—Å—Å, –∫–æ—Ç–æ—Ä—ã–π –¥–µ–ª–∞–µ—Ç –Ω–∞—à–∏ –¥–∞–Ω–Ω—ã–µ —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–º–∏ —Å PyTorch-DataLoader‚Äô–æ–º.

1. –ù–∞—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –æ—Ç torch.utils.data.Dataset
–ü–æ–∑–≤–æ–ª—è–µ—Ç –Ω–∞—à–µ–º—É –∫–ª–∞—Å—Å—É —Ä–∞–±–æ—Ç–∞—Ç—å —Å –ª—é–±—ã–º–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏ PyTorch –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –ø–µ—Ä–µ–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö.

2.__init__(self, samples, targets)

samples ‚Äî –º–∞—Å—Å–∏–≤ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (numpy-–º–∞—Å—Å–∏–≤ —Ñ–æ—Ä–º—ã (N, T, H),) –ø–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏), 
–≥–¥–µ N = —á–∏—Å–ª–æ –æ–±—Ä–∞–∑—Ü–æ–≤, T = len(T_values), H = len(H_values).
–ß—Ç–æ–±—ã –ø–æ–¥–∞—Ç—å –º–∞—Å—Å–∏–≤ samples –≤ Conv2d, –Ω—É–∂–µ–Ω –µ—â—ë ¬´–∫–∞–Ω–∞–ª¬ª (channel) –ø–µ—Ä–µ–¥ T- –∏ H-–æ—Å—è–º–∏.
–¢–µ–ø–µ—Ä—å shape samples: (N, 1, T, H).
1 ‚Äî —ç—Ç–æ —á–∏—Å–ª–æ –∫–∞–Ω–∞–ª–æ–≤ (–∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ grayscale-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é –æ–¥–Ω–æ–≥–æ –∫–∞–Ω–∞–ª–∞).

targets ‚Äî –º–∞—Å—Å–∏–≤ —Ü–µ–ª–µ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π (N, 4) (—á–µ—Ç—ã—Ä–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ ùêΩ).
–í–Ω—É—Ç—Ä–∏ –º—ã –ø—Ä–µ–≤—Ä–∞—â–∞–µ–º –∏—Ö –≤ —Ç–µ–Ω–∑–æ—Ä—ã torch.Tensor —Å —Ç–∏–ø–æ–º float32, —á—Ç–æ–±—ã –¥–∞–ª—å—à–µ –ø–æ–¥–∞–≤–∞—Ç—å –≤ –Ω–µ–π—Ä–æ—Å–µ—Ç—å.

3.__len__(self)
–î–æ–ª–∂–µ–Ω –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å —á–∏—Å–ª–æ –æ–±—Ä–∞–∑—Ü–æ–≤ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ. 
PyTorch –≤—ã–∑—ã–≤–∞–µ—Ç –µ–≥–æ, —á—Ç–æ–±—ã –ø–æ–Ω—è—Ç—å, —Å–∫–æ–ª—å–∫–æ –≤—Å–µ–≥–æ –æ–±—Ä–∞–∑—Ü–æ–≤ –∏ –∫–æ–≥–¥–∞ –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å—Å—è.

4.__getitem__(self, idx)
–ü–æ –∏–Ω–¥–µ–∫—Å—É idx –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ—Ä—Ç–µ–∂ (sample, target).

self.samples[idx] –≤—ã–¥–∞—ë—Ç –æ–¥–Ω–æ–º–µ—Ä–Ω—ã–π —Ç–µ–Ω–∑–æ—Ä: sample.shape = (1, T, H),

self.targets[idx] ‚Äî —Ç–µ–Ω–∑–æ—Ä –¥–ª–∏–Ω—ã 4 —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ J. target.shape = (4,)
DataLoader –±–µ—Ä—ë—Ç —ç—Ç–æ—Ç –∫–æ—Ä—Ç–µ–∂ –∏ –≥—Ä—É–ø–ø–∏—Ä—É–µ—Ç –∏—Ö –≤ –±–∞—Ç—á–∏ –∑–∞–¥–∞–Ω–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞.

–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, MagnetizationDataset ‚Äî —ç—Ç–æ –ø—Ä–æ—Å—Ç–æ ¬´–æ–±—ë—Ä—Ç–∫–∞¬ª –Ω–∞–¥ –Ω–∞—à–∏–º–∏ –º–∞—Å—Å–∏–≤–∞–º–∏, 
–ø—Ä–µ–≤—Ä–∞—â–∞—é—â–∞—è –∏—Ö –≤ –∏—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏, –ø–æ–Ω—è—Ç–Ω—ã–π –¥–ª—è PyTorch.
'''
# class MagnetizationDataset(Dataset):
#     def __init__(self, samples, targets):
#         self.samples = torch.tensor(samples, dtype=torch.float32)
#         self.targets = torch.tensor(targets, dtype=torch.float32)
#
#     def __len__(self):
#         return len(self.samples)
#
#     def __getitem__(self, idx):
#         return self.samples[idx], self.targets[idx]

class MagnetizationDataset(Dataset):
    def __init__(self, samples, targets):
        # samples ‚Äî numpy-–º–∞—Å—Å–∏–≤ —Ñ–æ—Ä–º—ã (N, T, H),
        # –≥–¥–µ N = —á–∏—Å–ª–æ –∫–∞—Ä—Ç m(H,T), T = len(T_values), H = len(H_values).
        # –ß—Ç–æ–±—ã –ø–æ–¥–∞—Ç—å –µ–≥–æ –≤ Conv2d, –Ω—É–∂–µ–Ω –µ—â—ë ¬´–∫–∞–Ω–∞–ª¬ª (channel) –ø–µ—Ä–µ–¥ T- –∏ H-–æ—Å—è–º–∏.
        self.samples = torch.tensor(samples, dtype=torch.float32).unsqueeze(1)
        # –¢–µ–ø–µ—Ä—å shape samples: (N, 1, T, H).
        # 1 ‚Äî —ç—Ç–æ —á–∏—Å–ª–æ –∫–∞–Ω–∞–ª–æ–≤ (–∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ grayscale-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é –æ–¥–Ω–æ–≥–æ –∫–∞–Ω–∞–ª–∞).

        # targets ‚Äî —Ç–µ–Ω–∑–æ—Ä —Ñ–æ—Ä–º—ã (N, 4) —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ J –¥–ª—è –∫–∞–∂–¥–æ–π –∫–∞—Ä—Ç—ã.
        self.targets = torch.tensor(targets, dtype=torch.float32)

    def __len__(self):
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º N ‚Äî —Å–∫–æ–ª—å–∫–æ –≤—Å–µ–≥–æ –æ–±—Ä–∞–∑—Ü–æ–≤ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ
        return len(self.samples)

    def __getitem__(self, idx):
        # –ü–æ –∏–Ω–¥–µ–∫—Å—É –¥–∞—ë–º –ø–∞—Ä—É (sample, target):
        #   sample.shape = (1, T, H),
        #   target.shape = (4,)
        return self.samples[idx], self.targets[idx]


# ============================
# –ú–æ–¥–µ–ª—å
# ============================

#--------------NN architecture parameters (–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å –¥–ª—è —Å–ª—É—á–∞—è m(H)).
#  –ê –≤ —ç—Ç–æ–º —Å–∫—Ä–∏–ø—Ç–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å–ª—É—á–∞–π  2D –∫–∞—Ä—Ç—ã ùëö(ùêª,ùëá)
# input_size = len(H_values)
# hidden_size_1 = 256
# hidden_size_2 = 128
# output_size = 4
#
# DROPOUT_PARAMETER = 0.1
# WEIGHT_DECAY_PARAMETER = 1e-4



# class Net(nn.Module):
#     def __init__(self):
#         super(Net, self).__init__()
#         self.fc1 = nn.Linear(input_size, hidden_size_1)
#         self.bn1 = nn.BatchNorm1d(hidden_size_1)
#         self.fc2 = nn.Linear(hidden_size_1, hidden_size_2)
#         self.bn2 = nn.BatchNorm1d(hidden_size_2)
#         self.fc3 = nn.Linear(hidden_size_2, output_size)
#         self.dropout = nn.Dropout(DROPOUT_PARAMETER)
#         self.relu = nn.ReLU()
#
#     def forward(self, x):
#         x = self.fc1(x)
#         x = self.bn1(x)
#         x = self.relu(x)
#         x = self.dropout(x)
#         x = self.fc2(x)
#         x = self.bn2(x)
#         x = self.relu(x)
#         x = self.dropout(x)
#         x = self.fc3(x)
#         return x

#--------------NN architecture parameters for 2D-CNN
# –ß–∏—Å–ª–æ –≤—ã—Ö–æ–¥–Ω—ã—Ö –∫–∞–Ω–∞–ª–æ–≤ (—Ñ–∏–ª—å—Ç—Ä–æ–≤) –≤ –ø–µ—Ä–≤–æ–º —Å–≤—ë—Ä—Ç–æ—á–Ω–æ–º —Å–ª–æ–µ.
# –û—Ç–≤–µ—á–∞–µ—Ç –∑–∞ —Ç–æ, —Å–∫–æ–ª—å–∫–æ —Ä–∞–∑–Ω—ã—Ö ¬´–ø—Ä–∏–∑–Ω–∞–∫–æ–≤¬ª (features) —Å–µ—Ç—å –±—É–¥–µ—Ç –∏–∑–≤–ª–µ–∫–∞—Ç—å –∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –≤—Ö–æ–¥–∞.
CONV1_OUT            = 16

# –ß–∏—Å–ª–æ –≤—ã—Ö–æ–¥–Ω—ã—Ö –∫–∞–Ω–∞–ª–æ–≤ –≤–æ –≤—Ç–æ—Ä–æ–º —Å–≤—ë—Ä—Ç–æ—á–Ω–æ–º —Å–ª–æ–µ.
# –û–±—ã—á–Ω–æ –¥–µ–ª–∞—é—Ç –Ω–∞—Ä–∞—Å—Ç–∞–Ω–∏–µ —á–∏—Å–ª–∞ —Ñ–∏–ª—å—Ç—Ä–æ–≤ –ø–æ –º–µ—Ä–µ —É–≥–ª—É–±–ª–µ–Ω–∏—è —Å–µ—Ç–∏, —á—Ç–æ–±—ã –∑–∞—Ö–≤–∞—Ç—ã–≤–∞—Ç—å –≤—Å—ë –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏.
CONV2_OUT            = 32

# –†–∞–∑–º–µ—Ä —è–¥—Ä–∞ —Å–≤—ë—Ä—Ç–∫–∏ (3√ó3).
# –ú–∞–ª–µ–Ω—å–∫–∏–µ —è–¥—Ä–∞ (3√ó3) –ø–æ–∑–≤–æ–ª—è—é—Ç –ø—Ä–∏ –Ω–µ–±–æ–ª—å—à–æ–º —á–∏—Å–ª–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å—Ç—Ä–æ–∏—Ç—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –≥–ª—É–±–æ–∫–∏–µ –∏ –º–æ—â–Ω—ã–µ –º–æ–¥–µ–ª–∏.
KERNEL_SIZE          = 3

# ????? –†–∞–∑–º–µ—Ä –æ–∫–Ω–∞ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –ø—É–ª–∏–Ω–≥–∞ (2√ó2).
# –ü–æ—Å–ª–µ –∫–∞–∂–¥–æ–π —Å–≤—ë—Ä—Ç–∫–∏ –º—ã –ø–æ–Ω–∏–∂–∞–µ–º —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤–æ–π –∫–∞—Ä—Ç—ã –≤–¥–≤–æ–µ –ø–æ –æ–±–µ–∏–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–º –æ—Å—è–º.
POOL_KERNEL          = 2            # –º–∞—Å—à—Ç–∞–± –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –ø—É–ª–∏–Ω–≥–∞

# ????? –°–∫–æ–ª—å–∫–æ —Ä–∞–∑ –ø–æ–¥—Ä—è–¥ –º—ã –ø—Ä–∏–º–µ–Ω—è–µ–º –ø—É–ª (—Ç.‚Ää–µ. –¥–≤–∞ —Ä–∞–∑–∞ –ø–æ–¥—Ä—è–¥ –ø–æ 2√ó2).
# –í –∏—Ç–æ–≥–µ –∏ –ø–æ ¬´T¬ª (—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å), –∏ –ø–æ ¬´H¬ª (–ø–æ–ª–µ) –∫–∞—Ä—Ç–∏–Ω–∫–∞ —É–º–µ–Ω—å—à–∞–µ—Ç—Å—è –≤ 2¬≤=4 —Ä–∞–∑–∞.
POOL_STEPS           = 2            # —á–∏—Å–ª–æ –ø–æ–¥—Ä—è–¥ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–π pool

# –†–∞–∑–º–µ—Ä —Å–∫—Ä—ã—Ç–æ–≥–æ (–ø–µ—Ä–≤–æ–≥–æ) –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω–æ–≥–æ —Å–ª–æ—è –≤ ¬´–¥–µ–∫–æ–¥–µ—Ä–µ¬ª.
# –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —Å–∫–æ–ª—å–∫–æ –Ω–µ–π—Ä–æ–Ω–æ–≤ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç ¬´—Å–ø–ª—é—â–µ–Ω–Ω—É—é¬ª –∫–∞—Ä—Ç—É –ø–µ—Ä–µ–¥ —Ñ–∏–Ω–∞–ª—å–Ω—ã–º –≤—ã—Ö–æ–¥–æ–º.
FC_HIDDEN            = 128

# –î–æ–ª—è –Ω–µ–π—Ä–æ–Ω–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ –æ–±—É—á–µ–Ω–∏—è —Å–ª—É—á–∞–π–Ω–æ ¬´–≤—ã–∫–ª—é—á–∞—é—Ç—Å—è¬ª (dropout).
# –ü–æ–º–æ–≥–∞–µ—Ç –±–æ—Ä–æ—Ç—å—Å—è —Å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ–º, –∑–∞—Å—Ç–∞–≤–ª—è—è —Å–µ—Ç—å –Ω–µ —Å–ª–∏—à–∫–æ–º —Å–∏–ª—å–Ω–æ –ø–æ–ª–∞–≥–∞—Ç—å—Å—è –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω—ã–µ –Ω–µ–π—Ä–æ–Ω—ã.
DROPOUT_PARAMETER    = 0.1

# –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç L2-—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ –≤–µ—Å–æ–≤ (–ø–∞—Ä–∞–º–µ—Ç—Ä weight_decay –≤ Adam).
# –û–Ω –¥–æ–±–∞–≤–ª—è–µ—Ç —à—Ç—Ä–∞—Ñ –∑–∞ –±–æ–ª—å—à–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤–µ—Å–æ–≤ –∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ.
WEIGHT_DECAY_PARAMETER = 1e-4

# –†–∞–∑–º–µ—Ä –≤—ã—Ö–æ–¥–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞ ‚Üí –º—ã –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º —Å—Ä–∞–∑—É —á–µ—Ç—ã—Ä–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ (J1, J2, J3, J4).
output_size          = 4


'''
–¢–ò–ü –ê–†–•–ò–¢–ï–ö–¢–£–†–´:
—Å–≤–µ—Ä—Ç–æ—á–Ω—ã–π —Ä–µ–≥—Ä–µ—Å—Å–æ—Ä (CNN regressor) –≤ ¬´—ç–Ω–∫–æ–¥–µ—Ä-–¥–µ–∫–æ–¥–µ—Ä–Ω–æ–º¬ª —Å—Ç–∏–ª–µ, 
–Ω–æ —ç—Ç–æ –Ω–µ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä, –ø–æ—Ç–æ–º—É —á—Ç–æ –≤—ã –Ω–µ –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç–µ –≤—Ö–æ–¥. 

–°–∫–æ—Ä–µ–µ –æ–Ω —Ç–∞–∫ –∏ –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è:
Convolutional encoder + FC-head regressor,
–∏–ª–∏ –ø—Ä–æ—Å—Ç–æ CNN-based regression network.
'''
class Net2D(nn.Module):
    def __init__(self):
        super().__init__()
        # =======1) –í—ã—á–∏—Å–ª–µ–Ω–∏–µ pad==========
        # –ß—Ç–æ–±—ã —è–¥—Ä–æ 3√ó3 —Å–æ—Ö—Ä–∞–Ω—è–ª–æ —Ç—É –∂–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—É—é —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å (—Å ¬´–ø–∞–¥–∞–º–∏¬ª),
        # –º—ã —Å—Ç–∞–≤–∏–º –æ—Ç—Å—Ç—É–ø –≤ 1 –ø–∏–∫—Å–µ–ª—å (3//2).
        pad = KERNEL_SIZE // 2

        # =======2) C–≤–µ—Ä—Ç–æ—á–Ω–∞—è ¬´—ç–Ω–∫–æ–¥–µ—Ä¬ª-—á–∞—Å—Ç—å
        '''
        Conv2d(1, CONV1_OUT): 
        –∏–∑ –æ–¥–Ω–æ–≥–æ –∫–∞–Ω–∞–ª–∞ (—Ç—É—Ç —É –Ω–∞—Å –∫–∞–Ω–∞–ª ‚Äî —ç—Ç–æ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–Ω–æ-–ø–æ–ª–µ–≤–∞—è –∫–∞—Ä—Ç–∞) —Å—Ç—Ä–æ–∏–º CONV1_OUT –ø—Ä–∏–∑–Ω–∞–∫–æ–≤—ã—Ö –∫–∞—Ä—Ç.

        BatchNorm2d: 
        –Ω–æ—Ä–º–∏—Ä—É–µ—Ç –∫–∞–∂–¥—É—é –∏–∑ CONV?_OUT –∫–∞—Ä—Ç –ø–æ –±–∞—Ç—á—É, —É—Å–∫–æ—Ä—è—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å.

        ReLU(): 
        –¥–æ–±–∞–≤–ª—è–µ—Ç –Ω–µ–ª–∏–Ω–µ–π–Ω–æ—Å—Ç—å.

        MaxPool2d(POOL_KERNEL): 
        –ø–æ–Ω–∏–∂–∞–µ—Ç —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ –≤–¥–≤–æ–µ.

        –ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ –¥–ª—è –≤—Ç–æ—Ä–æ–≥–æ —Å–≤—ë—Ä—Ç–æ—á–Ω–æ–≥–æ —Å–ª–æ—è.
        '''
        self.encoder = nn.Sequential(
            nn.Conv2d(1, CONV1_OUT, kernel_size=KERNEL_SIZE, padding=pad),
            nn.BatchNorm2d(CONV1_OUT),
            nn.ReLU(),
            nn.MaxPool2d(POOL_KERNEL),

            nn.Conv2d(CONV1_OUT, CONV2_OUT, kernel_size=KERNEL_SIZE, padding=pad),
            nn.BatchNorm2d(CONV2_OUT),
            nn.ReLU(),
            nn.MaxPool2d(POOL_KERNEL),
        )

        # =======3) –í—ã—á–∏—Å–ª–µ–Ω–∏–µ flattened_size==========
        # –≤—ã—á–∏—Å–ª—è–µ–º –∏—Ç–æ–≥–æ–≤—ã–π —Ä–∞–∑–º–µ—Ä ¬´—Å–ø–ª—é—â–µ–Ω–Ω–æ–≥–æ¬ª —Ç–µ–Ω–∑–æ—Ä–∞
        t_dim = T_POINTS_NUMBER // (POOL_KERNEL**POOL_STEPS)
        h_dim = H_POINTS_NUMBER // (POOL_KERNEL**POOL_STEPS)
        flattened_size = CONV2_OUT * t_dim * h_dim

        # =======2) –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω–∞—è ¬´–¥–µ–∫–æ–¥–µ—Ä¬ª-—á–∞—Å—Ç—å
        '''
        Linear(flattened_size, FC_HIDDEN): 
        –ø—Ä–æ–µ–∫—Ü–∏—è –∏–∑ –æ—á–µ–Ω—å –±–æ–ª—å—à–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ —Ä–∞–∑–º–µ—Ä–æ–º FC_HIDDEN.

        ReLU() –∏ Dropout: 
        –Ω–µ–ª–∏–Ω–µ–π–Ω–æ—Å—Ç—å + —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è.

        Linear(FC_HIDDEN, output_size): 
        —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Å–ª–æ–π –±–µ–∑ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏, –¥–∞—é—â–∏–π —á–µ—Ç—ã—Ä—ë—Ö–º–µ—Ä–Ω—ã–π –≤–µ–∫—Ç–æ—Ä –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.
        '''
        self.decoder = nn.Sequential(
            nn.Linear(flattened_size, FC_HIDDEN),
            nn.ReLU(),
            nn.Dropout(DROPOUT_PARAMETER),
            nn.Linear(FC_HIDDEN, output_size),
        )

    '''
    self.encoder(x):
    –ò–∑ –≤—Ö–æ–¥–∞ (batch, 1, T, H) –ø–æ–ª—É—á–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–æ–≤—ã–µ –∫–∞—Ä—Ç—ã (batch, CONV2_OUT, T/4, H/4).

    x.flatten(start_dim=1):
    –°–≥–ª–∞–∂–∏–≤–∞–µ–º –≤—Å–µ, –∫—Ä–æ–º–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –±–∞—Ç—á–∞, –≤ –æ–¥–∏–Ω –¥–ª–∏–Ω–Ω—ã–π –≤–µ–∫—Ç–æ—Ä –¥–ª–∏–Ω—ã flattened_size.

    self.decoder(x):
    –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —á–µ—Ä–µ–∑ –¥–≤–∞ –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã—Ö —Å–ª–æ—è –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ (batch, 4).
    '''
    def forward(self, x):
        # x.shape = (batch, 1, T, H)
        x = self.encoder(x)          # ‚Üí (batch, CONV2_OUT, T/4, H/4)
        x = x.flatten(start_dim=1)   # ‚Üí (batch, flattened_size)
        return self.decoder(x)       # ‚Üí (batch, output_size)


# 3) –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤ ¬´—Å—ã—Ä—ã—Ö¬ª –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–µ–π - –≥—Ä–∞—Ñ–∏–∫ m(H,T) –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö/–≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö/—Ç–µ—Å—Ç–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤
def plot_raw_map(map_raw, J_params, idx, split_name):
    plt.figure(figsize=(5,4))
    plt.imshow(map_raw,
               origin='lower',
               extent = [H_MIN, H_MAX, T_MIN, T_MAX],
               aspect = 'auto')
    plt.colorbar(label='m(H,T)')
    plt.title(f"{split_name} #{idx}, J={J_params}")
    plt.xlabel('H')
    plt.ylabel('T')
    plt.tight_layout()
    plt.savefig(f"{split_name.lower()}_mHT_raw_{idx}.png")
    plt.close()

# ============================
# –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è
# ============================

def main():
    logging.basicConfig(level=logging.INFO)

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
    logging.info("Generating datasets...")
    # –ú—ã —Ö–æ—Ç–∏–º –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –≥—Ä–∞—Ñ–∏–∫–∏ (–ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–∏) –Ω–∞–º–∞–≥–Ω–∏—á–µ–Ω–Ω–æ—Å—Ç–∏ –Ω–∞ –∫–æ—Ç–æ—Ä—ã—Ö –ø—Ä–æ–≤–æ–¥–∏—Ç—Å—è –æ–±—É—á–µ–Ω–∏–µ, –≤–∞–ª–∏–¥–∞—Ü–∏—è –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ.
    # –î–ª—è —ç—Ç–æ–≥–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º _raw 2–î –∫–∞—Ä—Ç—ã (–Ω–µ–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ).
    # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ 2–î –∫–∞—Ä—Ç—ã –Ω—É–∂–Ω—ã –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π.
    # –ù–µ–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ 2–î –∫–∞—Ä—Ç—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–π —Å—Ç–∞—Ç—å–µ, –æ–Ω–∏ –∏–º–µ—é—Ç —Ñ–∏–∑.—Å–º—ã—Å–ª.

    # 1) –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–≤–∞ –Ω–∞–±–æ—Ä–∞: raw (train_X_raw) –∏ normalized (train_X)
    train_X_raw, train_Y = [], []
    val_X_raw, val_Y = [], []
    test_X_raw, test_Y = [], []

    for _ in range(TRAIN_SAMPLES_NUMBER):
        J_params = random_J(LATTICE_TYPE)
        # mag_curve = generate_sample(J_params)
        mag_curve = generate_sample_2d(J_params)
        train_X_raw.append(mag_curve)
        train_Y.append(J_params)

    for _ in range(VAL_SAMPLES_NUMBER):
        J_params = random_J(LATTICE_TYPE)
        # mag_curve = generate_sample(J_params)
        mag_curve = generate_sample_2d(J_params)
        val_X_raw.append(mag_curve)
        val_Y.append(J_params)

    for _ in range(TEST_SAMPLES_NUMBER):
        J_params = random_J(LATTICE_TYPE)
        # mag_curve = generate_sample(J_params)
        mag_curve = generate_sample_2d(J_params)
        test_X_raw.append(mag_curve)
        test_Y.append(J_params)

    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ numpy
    train_X_raw = np.array(train_X_raw) # shape=(N, T_points, H_points)
    val_X_raw = np.array(val_X_raw)
    test_X_raw = np.array(test_X_raw)
    train_Y = np.array(train_Y)
    val_Y = np.array(val_Y)
    test_Y = np.array(test_Y)

    # 2) –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    train_X = normalize_2d(train_X_raw)
    val_X = normalize_2d(val_X_raw)
    test_X = normalize_2d(test_X_raw)

    # --- –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–º–µ—Ä–æ–≤ ---
    # –í—ã–±–∏—Ä–∞–µ–º –æ–¥–∏–Ω –∏–Ω–¥–µ–∫—Å –∏–∑ –∫–∞–∂–¥–æ–≥–æ –Ω–∞–±–æ—Ä–∞ (–º–æ–∂–Ω–æ —Å–ª—É—á–∞–π–Ω—ã–π)
    i_train = np.random.randint(len(train_X))
    i_val = np.random.randint(len(val_X))
    i_test = np.random.randint(len(test_X))

    plot_raw_map(train_X_raw[i_train], train_Y[i_train], "Train", i_train)
    plot_raw_map(val_X_raw[i_val],     val_Y[i_val],     "Val",   i_val)
    plot_raw_map(test_X_raw[i_test],   test_Y[i_test],   "Test",  i_test)

    train_dataset = MagnetizationDataset(train_X, train_Y)
    val_dataset = MagnetizationDataset(val_X, val_Y)
    test_dataset = MagnetizationDataset(test_X, test_Y)

    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE_PARAMETER, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE_PARAMETER, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE_PARAMETER, shuffle=False)

    # ======= sanity checks ========
    batch_X, batch_Y = next(iter(train_loader))
    assert batch_X.shape == (BATCH_SIZE_PARAMETER, 1, T_POINTS_NUMBER, H_POINTS_NUMBER), \
        f"Expected input shape {(BATCH_SIZE_PARAMETER,1,T_POINTS_NUMBER,H_POINTS_NUMBER)}, got {batch_X.shape}"
    assert batch_Y.shape == (BATCH_SIZE_PARAMETER, 4), \
        f"Expected target shape {(BATCH_SIZE_PARAMETER,4)}, got {batch_Y.shape}"
    # ===============================

    #model = Net()
    model = Net2D()
    # –∑–∞–¥–∞—ë–º —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å (loss-—Ñ—É–Ω–∫—Ü–∏—é) –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏.
    # SmoothL1Loss (—Ç–∞–∫–∂–µ –∏–∑–≤–µ—Å—Ç–Ω–∞ –∫–∞–∫ Huber-loss)
    # —Å–æ—á–µ—Ç–∞–µ—Ç –≤ —Å–µ–±–µ
    # ¬´L1-—à—É–º–æ—É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å¬ª (–¥–ª—è –∫—Ä—É–ø–Ω—ã—Ö –æ—à–∏–±–æ–∫) –∏
    # ¬´L2-–ø–ª–∞–≤–Ω–æ—Å—Ç—å¬ª (–¥–ª—è –º–µ–ª–∫–∏—Ö),
    # —á—Ç–æ —á–∞—Å—Ç–æ –¥–∞—ë—Ç –±–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –≤—ã–±—Ä–æ—Å–∞—Ö, —á–µ–º –ø—Ä–æ—Å—Ç–æ MSE –∏–ª–∏ MAE.
    criterion = nn.SmoothL1Loss()

    # –≤—ã–±–∏—Ä–∞–µ–º –∞–ª–≥–æ—Ä–∏—Ç–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ Adam, –∫–æ—Ç–æ—Ä—ã–π –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç —Ç–µ–º–ø –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞.
    # weight_decay ‚Äî L2-—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è (—à—Ç—Ä–∞—Ñ –∑–∞ –±–æ–ª—å—à–∏–µ –≤–µ—Å–∞), –ø–æ–º–æ–≥–∞–µ—Ç –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—Ç–∏—Ç—å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ.
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE_PARAMETER, weight_decay=WEIGHT_DECAY_PARAMETER)

    # —Å–æ–∑–¥–∞—ë–º ¬´—à–µ–¥—É–ª–µ—Ä¬ª —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è:
    # –∫–∞–∂–¥—ã–µ step_size=50 —ç–ø–æ—Ö —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è –±—É–¥–µ—Ç —É–º–Ω–æ–∂–∞—Ç—å—Å—è –Ω–∞ gamma=0.5 (—Ç. –µ. —Å–Ω–∏–∂–∞—Ç—å—Å—è –≤–¥–≤–æ–µ).
    # –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –Ω–∞ —Å—Ç–∞—Ä—Ç–µ –±—ã—Å—Ç—Ä–æ —Å—Ö–æ–¥–∏—Ç—å—Å—è, –∞ –ø–æ –º–µ—Ä–µ –ø—Ä–∏–±–ª–∏–∂–µ–Ω–∏—è –∫ –º–∏–Ω–∏–º—É–º—É –¥–µ–ª–∞—Ç—å –º–µ–Ω—å—à–∏–µ —à–∞–≥–∏ –∏ —Ç–æ—á–Ω–µ–µ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å –≤–µ—Å–∞.
    scheduler = StepLR(optimizer, step_size=50, gamma=0.5)

    train_losses, val_losses = [], []

    logging.info("Start training...")
    for epoch in range(num_epochs):

        '''
        –ò—Ç–∞–∫, –∫–∞–∂–¥–∞—è —ç–ø–æ—Ö–∞ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑:
            1. –¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ –Ω–∞ –≤—Å–µ—Ö –±–∞—Ç—á–∞—Ö (—Å –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ–º –≤–µ—Å–æ–≤).
            2. –í–∞–ª–∏–¥–∞—Ü–∏—è (–±–µ–∑ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –≤–µ—Å–æ–≤).
            3. –ö–æ—Ä—Ä–µ–∫—Ü–∏—è —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—É—â–µ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏.
        '''

        # ------------ 1. –¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–π —à–∞–≥
        model.train()
        running_loss = 0
        for batch_X, batch_Y in train_loader:
            # batch_X ‚Äî –±–∞—Ç—á –≤—Ö–æ–¥–Ω—ã—Ö –∫—Ä–∏–≤—ã—Ö, batch_Y ‚Äî –±–∞—Ç—á –∏—Ö —Ü–µ–ª–µ–≤—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.

            # –æ–±–Ω—É–ª—è–µ–º –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –≤–æ –≤—Å–µ—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–∞—Ö (–∏–Ω–∞—á–µ –æ–Ω–∏ –±—ã —Å—É–º–º–∏—Ä–æ–≤–∞–ª–∏—Å—å).
            optimizer.zero_grad()

            # –ø—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ (forward): –ø–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–µ—Ç–∏ –¥–ª—è –±–∞—Ç—á–∞.
            outputs = model(batch_X)

            # –≤—ã—á–∏—Å–ª—è–µ–º —Ç–µ–∫—É—â—É—é –æ—à–∏–±–∫—É (Smooth L1) –º–µ–∂–¥—É –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏ –∏ –∏—Å—Ç–∏–Ω–Ω—ã–º–∏ J.
            loss = criterion(outputs, batch_Y)

            # –æ–±—Ä–∞—Ç–Ω—ã–π –ø—Ä–æ—Ö–æ–¥: PyTorch –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã—á–∏—Å–ª—è–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –≤—Å–µ—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å–µ—Ç–∏.
            loss.backward()

            # –æ–±–Ω–æ–≤–ª—è–µ–º –≤–µ—Å–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –∏ —Ç–µ–∫—É—â–µ–≥–æ learning rate.
            optimizer.step()

            # –Ω–∞–∫–∞–ø–ª–∏–≤–∞–µ–º —Å—É–º–º—É loss‚Äô–æ–≤, —É–º–Ω–æ–∂–µ–Ω–Ω—ã—Ö –Ω–∞ —á–∏—Å–ª–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤ –±–∞—Ç—á–µ (—á—Ç–æ–±—ã –ø–æ—Ç–æ–º —É—Å—Ä–µ–¥–Ω–∏—Ç—å –ø–æ –≤—Å–µ–º –æ–±—Ä–∞–∑—Ü–∞–º).
            running_loss += loss.item() * batch_X.size(0)

        # –ø–æ—Å–ª–µ –≤—Å–µ—Ö –±–∞—Ç—á–µ–π –¥–µ–ª–∏–º –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—É—é —Å—É–º–º—É –Ω–∞ –æ–±—â–µ–µ —á–∏—Å–ª–æ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤,
        # –ø–æ–ª—É—á–∞—è —Å—Ä–µ–¥–Ω—é—é –æ—à–∏–±–∫—É –∑–∞ —ç–ø–æ—Ö—É, –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –µ—ë –≤ —Å–ø–∏—Å–æ–∫ train_losses.
        epoch_loss = running_loss / len(train_loader.dataset)
        train_losses.append(epoch_loss)

        # ------------ 2. –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π —à–∞–≥

        # –ø–µ—Ä–µ–≤–æ–¥–∏–º –º–æ–¥–µ–ª—å –≤ ¬´—Ä–µ–∂–∏–º –æ—Ü–µ–Ω–∫–∏¬ª: Dropout –æ—Ç–∫–ª—é—á–∞–µ—Ç—Å—è, BatchNorm –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏, –∞ –Ω–µ —Ç–µ–∫—É—â–∏–π –±–∞—Ç—á.
        model.eval()
        running_val_loss = 0

        # –û–±–æ—Ä–∞—á–∏–≤–∞–µ–º –≤ torch.no_grad(), —á—Ç–æ–±—ã –Ω–µ –Ω–∞–∫–∞–ø–ª–∏–≤–∞—Ç—å –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –∏ –Ω–µ —Ç—Ä–∞—Ç–∏—Ç—å –ø–∞–º—è—Ç—å
        with torch.no_grad():
            for batch_X, batch_Y in val_loader:
                outputs = model(batch_X)
                loss = criterion(outputs, batch_Y)
                running_val_loss += loss.item() * batch_X.size(0)

        # —É—Å—Ä–µ–¥–Ω—è–µ–º –ø–æ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–º—É –Ω–∞–±–æ—Ä—É –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ val_losses
        epoch_val_loss = running_val_loss / len(val_loader.dataset)
        val_losses.append(epoch_val_loss)

        # ------------ 3. –ö–æ—Ä—Ä–µ–∫—Ü–∏—è —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è
        # –æ–±–Ω–æ–≤–ª—è–µ–º learning rate —Å–æ–≥–ª–∞—Å–Ω–æ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—é (StepLR —É–º–µ–Ω—å—à–∏—Ç –µ–≥–æ –≤–¥–≤–æ–µ –∫–∞–∂–¥—ã–µ 50 —ç–ø–æ—Ö).
        scheduler.step()

        # ------------ 4. –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        # –≤—ã–≤–æ–¥–∏–º –≤ –∫–æ–Ω—Å–æ–ª—å –ø—Ä–æ–≥—Ä–µ—Å—Å: –Ω–æ–º–µ—Ä —Ç–µ–∫—É—â–µ–π —ç–ø–æ—Ö–∏ –∏ —Å—Ä–µ–¥–Ω–∏–µ –æ—à–∏–±–∫–∏ –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–µ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏, –æ–∫—Ä—É–≥–ª—ë–Ω–Ω—ã–µ –¥–æ 6 –∑–Ω–∞–∫–æ–≤.
        logging.info(f"Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_loss:.6f}, Val Loss: {epoch_val_loss:.6f}")

    torch.save(model.state_dict(), TRAINED_MODEL_FILE)
    logging.info(f"Model saved to {TRAINED_MODEL_FILE}")

    # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∫—Ä–∏–≤–æ–π –æ–±—É—á–µ–Ω–∏—è
    plt.figure()
    plt.plot(train_losses, label='Training Loss')
    plt.plot(val_losses, label='Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)
    plt.savefig(LEARNING_CURVE_PLOT_FILE)
    plt.show()

if __name__ == "__main__":
    main()
