import logging
import numpy as np
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torch.optim.lr_scheduler import StepLR

# –∏—Å–ø–æ–ª—å–∑—É–µ–º torch
# 1. –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –≥—Ä–∞—Ñ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
# PyTorch —Å—Ç—Ä–æ–∏—Ç –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–π –≥—Ä–∞—Ñ ¬´–Ω–∞ —Ö–æ–¥—É¬ª (eager execution).
#
# 2. –ü—Ä–æ—Å—Ç–æ—Ç–∞ –∏ ¬´–ø–∏—Ç–æ–Ω–∏—á–Ω–æ—Å—Ç—å¬ª
# API PyTorch –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –±–ª–∏–∑–æ–∫ –∫ —á–∏—Å—Ç–æ–º—É Python/Numpy-–∫–æ–¥—É.
#
# 3. –ë–æ–ª—å—à–æ–µ —Å–æ–æ–±—â–µ—Å—Ç–≤–æ –∏ —ç–∫–æ—Å–∏—Å—Ç–µ–º–∞
# PyTorch –Ω–∞ —Å–µ–≥–æ–¥–Ω—è ‚Äî –æ–¥–∏–Ω –∏–∑ —Å–∞–º—ã—Ö –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–æ–≤ –≤ –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–æ–π —Å—Ä–µ–¥–µ (–±–æ–ª–µ–µ 60 % –ø—É–±–ª–∏–∫–∞—Ü–∏–π ML/—Å—Ç–∞—Ç–µ–π –∏—Å–ø–æ–ª—å–∑—É—é—Ç –µ–≥–æ –¥–ª—è —Ä–µ–ø—Ä–æducibility), –∞ —Ç–∞–∫–∂–µ –∞–∫—Ç–∏–≤–Ω–æ –Ω–∞–±–∏—Ä–∞–µ—Ç –¥–æ–ª—é –≤ –∏–Ω–¥—É—Å—Ç—Ä–∏–∏.

from dl_decorated_paper import math_utils

# ============================
# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
# ============================

#--------------File names
TRAINED_MODEL_FILE = 'model_decorated_mH.pth'
LEARNING_CURVE_PLOT_FILE = 'learning_curve_decorated_mH.png'

#--------------PhysicalSystem parameters
COMPUTE_TYPE = 'analytical'
LATTICE_TYPE = 'decorated'

H_MIN = 0
H_MAX = 10
H_POINTS_NUMBER = 64
H_values = np.linspace(H_MIN, H_MAX, H_POINTS_NUMBER)

T = 1.0

J_MIN = -1
J_MAX = +1

# connection between code parameters and paper parameters
# J1=Jd
# J2=Jd
# J3=J
# J4=Jt

# lattice types
# 'general' -> J1, J2, J3, J4
# 'decorated' -> J1, J2=J1, J3, J4
# 'molecule' -> J1, J2=J1, J3=J1, J4=0
# 'simple' -> J1, J2=J1, J3=0, J4=J1

#--------------Training parameters
#–°–∫–æ–ª—å–∫–æ —Å–ª—É—á–∞–π–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ (–ø–∞—Ä ¬´–∫—Ä–∏–≤–∞—è m(H) ‚Üí J-–ø–∞—Ä–∞–º–µ—Ç—Ä—ã¬ª) –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç—Å—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.
TRAIN_SAMPLES_NUMBER = 50000

#–†–∞–∑–º–µ—Ä –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–∞, –Ω–∞ –∫–æ—Ç–æ—Ä–æ–º –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ—Ç—Å—è ¬´–æ–±—â–∞—è¬ª –æ—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ (–Ω–µ —É—á–∞—Å—Ç–≤—É–µ—Ç –≤ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–º —Å–ø—É—Å–∫–µ).
VAL_SAMPLES_NUMBER = 5000

#–†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–∞, –∫–æ—Ç–æ—Ä—ã–º –º—ã –æ—Ü–µ–Ω–∏–≤–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –ø–æ—Å–ª–µ –æ–∫–æ–Ω—á–∞–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è.
TEST_SAMPLES_NUMBER = 5000

#–°–∫–æ–ª—å–∫–æ —Ü–µ–ª—ã—Ö –ø—Ä–æ—Ö–æ–¥–æ–≤ –ø–æ –≤—Å–µ–º—É —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–º—É –Ω–∞–±–æ—Ä—É –¥–∞–Ω–Ω—ã—Ö –±—É–¥–µ—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–æ.
num_epochs = 300

#–†–∞–∑–º–µ—Ä –º–∏–Ω–∏-–±–∞—Ç—á–∞ ‚Äî —Å–∫–æ–ª—å–∫–æ –æ–±—Ä–∞–∑—Ü–æ–≤ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –ø—Ä–æ–≥–æ–Ω—è–µ—Ç—Å—è —á–µ—Ä–µ–∑ —Å–µ—Ç—å –ø–µ—Ä–µ–¥ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ–º –≤–µ—Å–æ–≤.
BATCH_SIZE_PARAMETER = 64

#–ù–∞—á–∞–ª—å–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è (—à–∞–≥ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞) –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ Adam.
LEARNING_RATE_PARAMETER = 0.001
'''
                    –ü—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è
1. –£ –Ω–∞—Å –µ—Å—Ç—å TRAIN_SAMPLES_NUMBER –æ–±—Ä–∞–∑—Ü–æ–≤.
2. –ú—ã —Ä–∞–∑–±–∏–≤–∞–µ–º –∏—Ö –Ω–∞ –º–∏–Ω–∏-–±–∞—Ç—á–∏ –ø–æ BATCH_SIZE_PARAMETER —à—Ç—É–∫.
3. –î–ª—è –∫–∞–∂–¥–æ–≥–æ –±–∞—Ç—á–∞ –º—ã:

–¥–µ–ª–∞–µ–º forward (–ø—Ä–æ–≥–æ–Ω —á–µ—Ä–µ–∑ —Å–µ—Ç—å),
–≤—ã—á–∏—Å–ª—è–µ–º loss,
backward (–≥—Ä–∞–¥–∏–µ–Ω—Ç—ã) –∏
optimizer.step() (–æ–±–Ω–æ–≤–ª—è–µ–º –≤–µ—Å–∞).

4. –ö–æ–≥–¥–∞ –º—ã –ø–µ—Ä–µ–±—Ä–∞–ª–∏ –≤—Å–µ –±–∞—Ç—á–∏ (—Ç–æ –µ—Å—Ç—å –ø—Ä–æ—à–ª–∏ –ø–æ –≤—Å–µ–º TRAIN_SAMPLES_NUMBER –æ–±—Ä–∞–∑—Ü–∞–º),
—ç—Ç–æ –∑–∞–≤–µ—Ä—à–∞–µ—Ç –æ–¥–Ω—É —ç–ø–æ—Ö—É.
5. –ó–∞—Ç–µ–º –º—ã —Å–Ω–æ–≤–∞ –Ω–∞—á–∏–Ω–∞–µ–º –ø–µ—Ä–µ–±–æ—Ä –±–∞—Ç—á–µ–π —Å –ø–µ—Ä–≤–æ–≥–æ, –∏ —Ç–∞–∫ –ø–æ–≤—Ç–æ—Ä—è–µ–º num_epochs —Ä–∞–∑.

–ï—Å–ª–∏ –ø–æ—Å–ª–µ–¥–Ω–∏–π –±–∞—Ç—á —Ä–∞–∑–º–µ—Ä–æ–º –º–µ–Ω—å—à–µ BATCH_SIZE_PARAMETER, –æ–Ω —Ç–æ–∂–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è —Ç–æ—á–Ω–æ —Ç–∞–∫ –∂–µ.


–í –¥–∞–Ω–Ω–æ–º –∫–æ–¥–µ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ train_loader –º—ã –ø–µ—Ä–µ–¥–∞—ë–º shuffle=True:

train_loader = DataLoader(train_dataset,
                          batch_size=BATCH_SIZE_PARAMETER,
                          shuffle=True)
–≠—Ç–æ –∑–Ω–∞—á–∏—Ç, —á—Ç–æ –ø—Ä–∏ –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–µ PyTorch –±—É–¥–µ—Ç –ø–µ—Ä–µ–º–µ—à–∏–≤–∞—Ç—å –≤–µ—Å—å —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –ø–µ—Ä–µ–¥ —Ç–µ–º, –∫–∞–∫ —Ä–∞–∑–±–∏—Ç—å –µ–≥–æ –Ω–∞ –±–∞—Ç—á–∏. –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ:

–ë–∞—Ç—á–∏ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —ç–ø–æ—Ö–∞—Ö –±—É–¥—É—Ç —Å–æ—Å—Ç–æ—è—Ç—å –∏–∑ —Å–ª—É—á–∞–π–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –æ–±—Ä–∞–∑—Ü–æ–≤.
–í –æ–¥–Ω–æ–º –∏ —Ç–æ–º –∂–µ –±–∞—Ç—á–µ –≤ –¥–≤—É—Ö —Ä–∞–∑–Ω—ã—Ö —ç–ø–æ—Ö–∞—Ö –º–æ–≥—É—Ç –æ–∫–∞–∑–∞—Ç—å—Å—è —Ä–∞–∑–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã.

–î–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∏ —Ç–µ—Å—Ç–∞ –º—ã –æ–±—ã—á–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º shuffle=False, —á—Ç–æ–±—ã –æ—Ü–µ–Ω–∫–∏ –±—ã–ª–∏ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º—ã –∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã:

val_loader   = DataLoader(val_dataset, batch_size=‚Ä¶, shuffle=False)
test_loader  = DataLoader(test_dataset, batch_size=‚Ä¶, shuffle=False)
'''

#--------------Monte-Carlo parameters (for generation of Dataset)
#–í —Ç–µ–∫—É—â–µ–º —Å–∫—Ä–∏–ø—Ç–µ —ç—Ç–∏ 3 –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è, –Ω–æ –ø—Ä–∏–≥–æ–¥—è—Ç—Å—è, –µ—Å–ª–∏ –±—É–¥–µ–º –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∫—Ä–∏–≤—ã–µ –º–µ—Ç–æ–¥–æ–º –ú–ö –≤–º–µ—Å—Ç–æ –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–æ–π —Ñ—É–Ω–∫—Ü–∏–∏.
#—á–∏—Å–ª–æ —Å–ø–∏–Ω–æ–≤ (—è—á–µ–µ–µ–∫) –≤ —Ü–µ–ø–æ—á–∫–µ
num_cells = 20

#–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ ¬´—Ä–∞—Å–∫—Ä—É—Ç–∫–∏¬ª —Å–∏—Å—Ç–µ–º—ã –¥–æ —Ä–∞–≤–Ω–æ–≤–µ—Å–∏—è
num_steps = 10000

#—á–∏—Å–ª–æ —à–∞–≥–æ–≤ –∏–∑–º–µ—Ä–µ–Ω–∏—è –ø–æ—Å–ª–µ —ç–∫–≤–∏–ª–∏–±—Ä–æ–≤–∫–∏
equil_steps = 500


'''
                    Monte-Carlo –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞–ø–∞ 
                    (–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞–ø—É –∏–∑ —Ç–æ—á–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–æ–π —Ñ–æ—Ä–º—É–ª—ã)
                    
–ü–æ—è—Å–Ω–∏–º –∑–Ω–∞—á–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.
1) equil_steps (—ç–∫–≤–∏–ª–∏–±—Ä–æ–≤–∫–∞, warm-up)

–ü—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —á–∏—Å–ª–æ MC-—à–∞–≥–æ–≤ (–∏—Ç–µ—Ä–∞—Ü–∏–π), –∫–æ—Ç–æ—Ä—ã–µ –º—ã –≤—ã–ø–æ–ª–Ω—è–µ–º –¥–æ —Ç–æ–≥–æ, 
–∫–∞–∫ –Ω–∞—á–∏–Ω–∞—Ç—å —Å–æ–±–∏—Ä–∞—Ç—å –∏–∑–º–µ—Ä–µ–Ω–∏—è.

–¶–µ–ª—å ‚Äî ¬´—Ä–∞—Å–∫—Ä—É—Ç–∏—Ç—å¬ª —Å–∏—Å—Ç–µ–º—É –∏–∑ —Å–ª—É—á–∞–π–Ω–æ–≥–æ –Ω–∞—á–∞–ª—å–Ω–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è, 
—á—Ç–æ–±—ã –æ–Ω–∞ –ø—Ä–∏—à–ª–∞ –≤ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞–≤–Ω–æ–≤–µ—Å–∏–µ –ø—Ä–∏ –∑–∞–¥–∞–Ω–Ω—ã—Ö ùêª,ùëá,ùêΩ.

–ë–µ–∑ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–π —ç–∫–≤–∏–ª–∏–±—Ä–æ–≤–∫–∏ –≤—ã —Ä–∏—Å–∫—É–µ—Ç–µ –∏–∑–º–µ—Ä—è—Ç—å –º–æ–º–µ–Ω—Ç–∞–ª—å–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ, 
–µ—â—ë –Ω–µ ¬´—É—Å—Ç–æ—è–≤—à–∏–µ—Å—è¬ª –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏, —á—Ç–æ –¥–∞—Å—Ç –∏—Å–∫–∞–∂—ë–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã.

2) num_steps (—Å–±–æ—Ä–∫–∞ –∏–∑–º–µ—Ä–µ–Ω–∏–π, sampling)

–≠—Ç–æ —á–∏—Å–ª–æ MC-—à–∞–≥–æ–≤ –ø–æ—Å–ª–µ —ç–∫–≤–∏–ª–∏–±—Ä–æ–≤–∫–∏, –≤ —Ö–æ–¥–µ –∫–æ—Ç–æ—Ä—ã—Ö –º—ã —Ñ–∏–∫c–∏—Ä—É–µ–º —Ç–µ–∫—É—â—É—é –Ω–∞–º–∞–≥–Ω–∏—á–µ–Ω–Ω–æ—Å—Ç—å 
(–∏/–∏–ª–∏ –¥—Ä—É–≥–∏–µ –Ω–∞–±–ª—é–¥–∞–µ–º—ã–µ) –¥–ª—è —É—Å—Ä–µ–¥–Ω–µ–Ω–∏—è.

–ß–µ–º –±–æ–ª—å—à–µ num_steps, —Ç–µ–º —Ç–æ—á–Ω–µ–µ –æ—Ü–µ–Ω–∫–∞ —Å—Ä–µ–¥–Ω–µ–≥–æ ùëö(ùêª,ùëá), 
–ø–æ—Ç–æ–º—É —á—Ç–æ —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ –∏–¥—ë—Ç –ø–æ –±–æ–ª—å—à–µ–º—É —á–∏—Å–ª—É —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π.

–û–±—ã—á–Ω–æ –º–µ–∂–¥—É –∏–∑–º–µ—Ä–µ–Ω–∏—è–º–∏ –¥–µ–ª–∞—é—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö MC-—à–∞–≥–æ–≤, 
—á—Ç–æ–±—ã —Å–Ω–∏–∑–∏—Ç—å –∞–≤—Ç–æ–∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é, –Ω–æ –≤ –ø—Ä–æ—Å—Ç–æ–º –≤–∞—Ä–∏–∞–Ω—Ç–µ –º–æ–∂–Ω–æ –∏–∑–º–µ—Ä—è—Ç—å –∫–∞–∂–¥—ã–π —à–∞–≥.
'''




# ============================
# –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
# ============================

def normalize(X):
    # X.shape = (N, H) ‚Äî N (–Ω–∞–ø—Ä–∏–º–µ—Ä N=TRAIN_SAMPLES_NUMBER) –æ–±—Ä–∞–∑—Ü–æ–≤, –∫–∞–∂–¥—ã–π –¥–ª–∏–Ω—ã H_POINTS_NUMBER (—á–∏—Å–ª–æ —Ç–æ—á–µ–∫ –ø–æ H_values)

    # —Å—á–∏—Ç–∞–µ–º —Å—Ä–µ–¥–Ω–µ–µ –¥–ª—è –∫–∞–∂–¥–æ–π –∫—Ä–∏–≤–æ–π
    mean = np.mean(X, axis=1, keepdims=True)
    # mean.shape = (N, 1)

    # —Å—á–∏—Ç–∞–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –¥–ª—è –∫–∞–∂–¥–æ–π –∫—Ä–∏–≤–æ–π
    std = np.std(X, axis=1, keepdims=True) + 1e-8
    # std.shape = (N, 1); +1e-8 ‚Äî —á—Ç–æ–±—ã –Ω–µ –¥–µ–ª–∏—Ç—å –Ω–∞ –Ω–æ–ª—å

    # —Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º –∫–∞–∂–¥–∞—è –∫—Ä–∏–≤–∞—è –ø–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –∏–º–µ–µ—Ç —Å—Ä–µ–¥–Ω–µ–µ 0 –∏ –¥–∏—Å–ø–µ—Ä—Å–∏—é 1
    return (X - mean) / std

# ============================
# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
# ============================

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–ª—É—á–∞–π–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ J –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ –æ—Ç J_MIN –¥–æ J_MAX
def random_J(lattice_type):
    if lattice_type == 'general':
        J1, J2, J3, J4 = np.random.uniform(J_MIN, J_MAX, 4)
        return J1, J2, J3, J4
    elif lattice_type == 'decorated':
        J1, J2, J3, J4 = np.random.uniform(J_MIN, J_MAX, 4)
        J2 = J1
        return J1, J2, J3, J4
    elif lattice_type == 'molecule':
        J1, J2, J3, J4 = np.random.uniform(J_MIN, J_MAX, 4)
        J2 = J1
        J3 = J1
        J4 = 0
        return J1, J2, J3, J4
    elif lattice_type == 'simple':
        J1, J2, J3, J4 = np.random.uniform(J_MIN, J_MAX, 4)
        J2 = J1
        J3 = 0
        J4 = J1
        return J1, J2, J3, J4
    else:
        J1, J2, J3, J4 =0,0,0,0
        return J1, J2, J3, J4

# m(H) for fixed T = 1.0
def generate_sample(J_params):
    J1, J2, J3, J4 = J_params
    Jd = J1
    J = J3
    Jt = J4
    mag_curve = [math_utils.m(J, Jd, Jt, H, T) for H in H_values]
    # –ù–∞ –≤—ã—Ö–æ–¥–µ –∏–º–µ–µ–º 1D –º–∞—Å—Å–∏–≤ –¥–ª–∏–Ω—ã H_POINTS_NUMBER - —Ç–æ—á–∫–∏ –Ω–∞ –∫—Ä–∏–≤–æ–π –Ω–∞–º–∞–≥–Ω–∏—á–µ–Ω–Ω–æ—Å—Ç–∏.
    # –≠–ª–µ–º–µ–Ω—Ç—ã –º–∞—Å—Å–∏–≤–∞ - –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞–º–∞–≥–Ω–∏—á–µ–Ω–Ω–æ—Å—Ç–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –ø–æ–ª—è–º –∏–∑ H_POINTS_NUMBER –ø—Ä–∏ –¥–∞–Ω–Ω—ã—Ö J,Jd,Jt –∏ T
    return np.array(mag_curve)



'''
                            MagnetizationDataset
–≠—Ç–æ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –∫–ª–∞—Å—Å, –∫–æ—Ç–æ—Ä—ã–π –¥–µ–ª–∞–µ—Ç –Ω–∞—à–∏ –¥–∞–Ω–Ω—ã–µ —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–º–∏ —Å PyTorch-DataLoader‚Äô–æ–º.

1. –ù–∞—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –æ—Ç torch.utils.data.Dataset
–ü–æ–∑–≤–æ–ª—è–µ—Ç –Ω–∞—à–µ–º—É –∫–ª–∞—Å—Å—É —Ä–∞–±–æ—Ç–∞—Ç—å —Å –ª—é–±—ã–º–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏ PyTorch –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –ø–µ—Ä–µ–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö.

2.__init__(self, samples, targets)

samples ‚Äî –º–∞—Å—Å–∏–≤ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (–º–∞—Ç—Ä–∏—Ü–∞ —Ñ–æ—Ä–º—ã (N, H_POINTS_NUMBER) –ø–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏), 
–≥–¥–µ N ‚Äî —á–∏—Å–ª–æ –ø—Ä–∏–º–µ—Ä–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä N=TRAIN_SAMPLES_NUMBER), H_POINTS_NUMBER ‚Äî –¥–ª–∏–Ω–∞ –∫—Ä–∏–≤–æ–π.

targets ‚Äî –º–∞—Å—Å–∏–≤ —Ü–µ–ª–µ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π (N, 4) (—á–µ—Ç—ã—Ä–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ ùêΩ).
–í–Ω—É—Ç—Ä–∏ –º—ã –ø—Ä–µ–≤—Ä–∞—â–∞–µ–º –∏—Ö –≤ —Ç–µ–Ω–∑–æ—Ä—ã torch.Tensor —Å —Ç–∏–ø–æ–º float32, —á—Ç–æ–±—ã –¥–∞–ª—å—à–µ –ø–æ–¥–∞–≤–∞—Ç—å –≤ –Ω–µ–π—Ä–æ—Å–µ—Ç—å.

3.__len__(self)
–î–æ–ª–∂–µ–Ω –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å —á–∏—Å–ª–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ. 
PyTorch –≤—ã–∑—ã–≤–∞–µ—Ç –µ–≥–æ, —á—Ç–æ–±—ã –ø–æ–Ω—è—Ç—å, —Å–∫–æ–ª—å–∫–æ –≤—Å–µ–≥–æ –æ–±—Ä–∞–∑—Ü–æ–≤ –∏ –∫–æ–≥–¥–∞ –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å—Å—è.

4.__getitem__(self, idx)
–ü–æ –∏–Ω–¥–µ–∫—Å—É idx –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ—Ä—Ç–µ–∂ (sample, target).

self.samples[idx] –≤—ã–¥–∞—ë—Ç –æ–¥–Ω–æ–º–µ—Ä–Ω—ã–π —Ç–µ–Ω–∑–æ—Ä –¥–ª–∏–Ω—ã H_POINTS_NUMBER.

self.targets[idx] ‚Äî —Ç–µ–Ω–∑–æ—Ä –¥–ª–∏–Ω—ã 4 —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ J.
DataLoader –±–µ—Ä—ë—Ç —ç—Ç–æ—Ç –∫–æ—Ä—Ç–µ–∂ –∏ –≥—Ä—É–ø–ø–∏—Ä—É–µ—Ç –∏—Ö –≤ –±–∞—Ç—á–∏ –∑–∞–¥–∞–Ω–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞.

–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, MagnetizationDataset ‚Äî —ç—Ç–æ –ø—Ä–æ—Å—Ç–æ ¬´–æ–±—ë—Ä—Ç–∫–∞¬ª –Ω–∞–¥ –Ω–∞—à–∏–º–∏ –º–∞—Å—Å–∏–≤–∞–º–∏, 
–ø—Ä–µ–≤—Ä–∞—â–∞—é—â–∞—è –∏—Ö –≤ –∏—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏, –ø–æ–Ω—è—Ç–Ω—ã–π –¥–ª—è PyTorch.
'''
class MagnetizationDataset(Dataset):
    def __init__(self, samples, targets):
        self.samples = torch.tensor(samples, dtype=torch.float32)
        self.targets = torch.tensor(targets, dtype=torch.float32)

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        return self.samples[idx], self.targets[idx]

# ============================
# –ú–æ–¥–µ–ª—å
# ============================

#--------------NN architecture parameters
# –†–∞–∑–º–µ—Ä –≤—Ö–æ–¥–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞. –í –Ω–∞—à–µ–º —Å–ª—É—á–∞–µ —ç—Ç–æ —á–∏—Å–ª–æ —Ç–æ—á–µ–∫ –Ω–∞ –∫—Ä–∏–≤–æ–π ùëö(ùêª), —Ç–æ –µ—Å—Ç—å 64.
input_size = len(H_values)

# –ß–∏—Å–ª–æ –Ω–µ–π—Ä–æ–Ω–æ–≤ –≤ –ø–µ—Ä–≤–æ–º —Å–∫—Ä—ã—Ç–æ–º (fully-connected) —Å–ª–æ–µ.
# –ß–µ–º –±–æ–ª—å—à–µ ‚Äî —Ç–µ–º –≤—ã—à–µ —ë–º–∫–æ—Å—Ç—å —Å–µ—Ç–∏, –Ω–æ –∏ —Ç–µ–º –ø—Ä–æ—â–µ –ø–µ—Ä–µ–æ–±—É—á–∏—Ç—å—Å—è.
hidden_size_1 = 256

# –ß–∏—Å–ª–æ –Ω–µ–π—Ä–æ–Ω–æ–≤ –≤–æ –≤—Ç–æ—Ä–æ–º —Å–∫—Ä—ã—Ç–æ–º —Å–ª–æ–µ.
# –û–±—ã—á–Ω–æ —ç—Ç–æ –º–µ–Ω—å—à–µ, —á–µ–º –≤ –ø–µ—Ä–≤–æ–º, —á—Ç–æ–±—ã ¬´—Å–∂–∞—Ç—å¬ª –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ.
hidden_size_2 = 128

# –†–∞–∑–º–µ—Ä –≤—ã—Ö–æ–¥–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞ —Å–µ—Ç–∏. –£ –Ω–∞—Å —á–µ—Ç—ã—Ä–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ ùêΩ1,ùêΩ2,ùêΩ3,ùêΩ4,
# –∫–æ—Ç–æ—Ä—ã–µ —Å–µ—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ.
output_size = 4

# –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å ¬´–≤—ã–∫–ª—é—á–µ–Ω–∏—è¬ª (drop) –Ω–µ–π—Ä–æ–Ω–∞ –Ω–∞ –∫–∞–∂–¥–æ–º —Å–ª–æ–µ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏.
# Dropout –ø–æ–º–æ–≥–∞–µ—Ç —Ä–µ–≥—É–ª—è—Ä–∏–∑–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å –∏ —Å–Ω–∏–∂–∞–µ—Ç –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ.
DROPOUT_PARAMETER = 0.1

# –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç L2-—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ –≤–µ—Å–æ–≤ (–ø–∞—Ä–∞–º–µ—Ç—Ä weight_decay –≤ Adam).
# –û–Ω –¥–æ–±–∞–≤–ª—è–µ—Ç —à—Ç—Ä–∞—Ñ –∑–∞ –±–æ–ª—å—à–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤–µ—Å–æ–≤ –∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ.
WEIGHT_DECAY_PARAMETER = 1e-4


'''
                            Net
–≠—Ç–æ –ø—Ä–æ—Å—Ç–∞—è –ø–æ–ª–Ω–æ—Å—Ç—å—é-—Å–≤—è–∑–Ω–∞—è (feed-forward) —Å–µ—Ç—å —Å –¥–≤—É–º—è —Å–∫—Ä—ã—Ç—ã–º–∏ —Å–ª–æ—è–º–∏ –∏ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π..

1. –ù–∞—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –æ—Ç nn.Module
–ü–æ–∑–≤–æ–ª—è–µ—Ç –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —É–ø—Ä–∞–≤–ª—è—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ (–≤–µ—Å–∞–º–∏, –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞–º–∏).

2. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–ª–æ—ë–≤

–ø–µ—Ä–≤—ã–π –ª–∏–Ω–µ–π–Ω—ã–π (fully-connected) —Å–ª–æ–π: –±–µ—Ä—ë—Ç –≤–µ–∫—Ç–æ—Ä –¥–ª–∏–Ω—ã input_size (64 —Ç–æ—á–µ–∫ –∫—Ä–∏–≤–æ–π ùëö(ùêª)) 
–∏ –ø—Ä–æ–µ—Ü–∏—Ä—É–µ—Ç –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ hidden_size_1 (256 –Ω–µ–π—Ä–æ–Ω–æ–≤). 
    self.fc1 = nn.Linear(input_size, hidden_size_1)


–±–∞—Ç—á-–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è: —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∞–∫—Ç–∏–≤–∞—Ü–∏–π –ø–æ—Å–ª–µ fc1, —É—Å–∫–æ—Ä—è–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –∏ –¥–∞—ë—Ç –ª—ë–≥–∫—É—é —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é.
    self.bn1 = nn.BatchNorm1d(hidden_size_1)

–≤—Ç–æ—Ä–æ–π –ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π: –∏–∑ 256 –Ω–µ–π—Ä–æ–Ω–æ–≤ –≤ 128, –ø–ª—é—Å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è.
    self.fc2 = nn.Linear(hidden_size_1, hidden_size_2)
    self.bn2 = nn.BatchNorm1d(hidden_size_2)

–≤—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π: –∏–∑ 128 –Ω–µ–π—Ä–æ–Ω–æ–≤ –≤ —á–µ—Ç—ã—Ä—ë—Ö–º–µ—Ä–Ω—ã–π –≤–µ–∫—Ç–æ—Ä {J1,J2,J3,J4}.
    self.fc3 = nn.Linear(hidden_size_2, output_size)
    
    
–ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ —Å–ª—É—á–∞–π–Ω–æ ¬´–≤—ã–∫–ª—é—á–∞–µ—Ç¬ª 10 % –Ω–µ–π—Ä–æ–Ω–æ–≤, —á—Ç–æ–±—ã —Å–Ω–∏–∑–∏—Ç—å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ.
    Dropout(0.1)
    
–Ω–µ–ª–∏–Ω–µ–π–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏, –æ–±–Ω—É–ª—è–µ—Ç –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ —Å–∫—Ä—ã—Ç—ã—Ö —Å–ª–æ—è—Ö.
–≠—Ç–æ –Ω—É–∂–Ω–æ —á—Ç–æ–±—ã —É–≤–µ–ª–∏—á–∏—Ç—å —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è.
    ReLU

3.–ú–µ—Ç–æ–¥ forward(self, x)
–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –∫–∞–∫ –¥–∞–Ω–Ω—ã–µ –ø—Ä–æ—Ö–æ–¥—è—Ç —á–µ—Ä–µ–∑ —Å–µ—Ç—å 

x = self.fc1(x)        # –ª–∏–Ω–µ–π–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ
x = self.bn1(x)        # –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ –±–∞—Ç—á—É
x = self.relu(x)       # –Ω–µ–ª–∏–Ω–µ–π–Ω–æ—Å—Ç—å
x = self.dropout(x)    # —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
x = self.fc2(x)        # –≤—Ç–æ—Ä–æ–π –ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π
x = self.bn2(x)        # –±–∞—Ç—á-–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
x = self.relu(x)       # ReLU
x = self.dropout(x)    # Dropout
x = self.fc3(x)        # –ª–∏–Ω–µ–π–Ω—ã–π –≤—ã—Ö–æ–¥
return x               # –±–µ–∑ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ ‚Äì —Ä–µ–≥—Ä–µ—Å—Å–∏—è


–î–≤–∞ —Å–∫—Ä—ã—Ç—ã—Ö —Å–ª–æ—è –ø–æ–∑–≤–æ–ª—è—é—Ç —Å–µ—Ç–∏ –≤—ã—É—á–∏—Ç—å —Å–ª–æ–∂–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æc—Ç–∏ –º–µ–∂–¥—É —Ñ–æ—Ä–º–æ–π –∫—Ä–∏–≤–æ–π m(H) –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ J.

BatchNorm + Dropout –≤ –∫–∞–∂–¥–æ–º –±–ª–æ–∫–µ —É–ª—É—á—à–∞—é—Ç —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –∏ —É–º–µ–Ω—å—à–∞—é—Ç –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ.

–û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –Ω–µ–ª–∏–Ω–µ–π–Ω–æ—Å—Ç–∏ –ø–æ—Å–ª–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–ª–æ—è (fc3) ‚Äî –ø–æ—Ç–æ–º—É —á—Ç–æ —ç—Ç–æ –∑–∞–¥–∞—á–∞ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ (–∞ –Ω–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ 0 –∏–ª–∏ 1), 
–∏ –º—ã —Ö–æ—Ç–∏–º –ø–æ–ª—É—á–∏—Ç—å –ª—é–±—ã–µ –≤–µ—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ –≤—ã—Ö–æ–¥–µ. 
–ê ReLU –±—ã "–æ–±—Ä–µ–∑–∞–ª–∞" –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ —á–∏—Å–ª–∞, –∞ —Å–∏–≥–º–æ–∏–¥ –≤—ã–¥–∞–ª–∞ –±—ã —á–∏—Å–ª–∞ –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ —Ç–æ–ª—å–∫–æ [0,1] 
'''

'''
–¢–ò–ü –ê–†–•–ò–¢–ï–ö–¢–£–†–´:
MLP-—Ä–µ–≥—Ä–µ—Å—Å–æ—Ä (multi-layer perceptron):

–Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–æ–ª–Ω–æ—Å—Ç—å—é-—Å–≤—è–∑–Ω—ã—Ö —Å–ª–æ—ë–≤ + BatchNorm + ReLU + Dropout,

–Ω–∞ –≤—ã—Ö–æ–¥–µ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è ‚Üí fully-connected regression network.
'''
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size_1)
        self.bn1 = nn.BatchNorm1d(hidden_size_1)
        self.fc2 = nn.Linear(hidden_size_1, hidden_size_2)
        self.bn2 = nn.BatchNorm1d(hidden_size_2)
        self.fc3 = nn.Linear(hidden_size_2, output_size)
        self.dropout = nn.Dropout(DROPOUT_PARAMETER)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.fc1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.fc2(x)
        x = self.bn2(x)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.fc3(x)
        return x

# 3) –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤ ¬´—Å—ã—Ä—ã—Ö¬ª –∫—Ä–∏–≤—ã—Ö - –≥—Ä–∞—Ñ–∏–∫ m(H) –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö/–≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö/—Ç–µ—Å—Ç–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤
def plot_raw_curve(X_raw, Y, idx, split_name):
    plt.figure(figsize=(6,4))
    plt.plot(H_values, X_raw[idx], 'o-', label=f"{split_name}, J={Y[idx]}")
    plt.xlabel("H")
    plt.ylabel("m(H, T=1)")
    plt.title(f"{split_name} sample #{idx} (raw)")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.savefig(f"{split_name.lower()}_mH_raw_example.png")
    plt.close()

# ============================
# –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è
# ============================

def main():
    logging.basicConfig(level=logging.INFO)

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
    logging.info("Generating datasets...")
    # –ú—ã —Ö–æ—Ç–∏–º –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –≥—Ä–∞—Ñ–∏–∫–∏ –Ω–∞–º–∞–≥–Ω–∏—á–µ–Ω–Ω–æ—Å—Ç–∏ –Ω–∞ –∫–æ—Ç–æ—Ä—ã—Ö –ø—Ä–æ–≤–æ–¥–∏—Ç—Å—è –æ–±—É—á–µ–Ω–∏–µ, –≤–∞–ª–∏–¥–∞—Ü–∏—è –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ.
    # –î–ª—è —ç—Ç–æ–≥–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º _raw –º–∞—Å—Å–∏–≤—ã (–Ω–µ–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ).
    # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –º–∞—Å—Å–∏–≤—ã –Ω—É–∂–Ω—ã –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π.
    # –ù–µ–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –º–∞—Å—Å–∏–≤—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–π —Å—Ç–∞—Ç—å–µ, –æ–Ω–∏ –∏–º–µ—é—Ç —Ñ–∏–∑.—Å–º—ã—Å–ª.

    # 1) –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–≤–∞ –Ω–∞–±–æ—Ä–∞: raw (train_X_raw) –∏ normalized (train_X)
    train_X_raw, train_Y = [], []
    val_X_raw, val_Y = [], []
    test_X_raw, test_Y = [], []

    for _ in range(TRAIN_SAMPLES_NUMBER):
        J_params = random_J(LATTICE_TYPE)
        mag_curve = generate_sample(J_params)
        train_X_raw.append(mag_curve)
        train_Y.append(J_params)

    for _ in range(VAL_SAMPLES_NUMBER):
        J_params = random_J(LATTICE_TYPE)
        mag_curve = generate_sample(J_params)
        val_X_raw.append(mag_curve)
        val_Y.append(J_params)

    for _ in range(TEST_SAMPLES_NUMBER):
        J_params = random_J(LATTICE_TYPE)
        mag_curve = generate_sample(J_params)
        test_X_raw.append(mag_curve)
        test_Y.append(J_params)

    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ numpy
    train_X_raw = np.array(train_X_raw)
    val_X_raw = np.array(val_X_raw)
    test_X_raw = np.array(test_X_raw)
    train_Y = np.array(train_Y)
    val_Y = np.array(val_Y)
    test_Y = np.array(test_Y)

    # 2) –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    train_X = normalize(train_X_raw)
    val_X = normalize(val_X_raw)
    test_X = normalize(test_X_raw)

    # --- –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–º–µ—Ä–æ–≤ ---
    # –í—ã–±–∏—Ä–∞–µ–º –æ–¥–∏–Ω –∏–Ω–¥–µ–∫—Å –∏–∑ –∫–∞–∂–¥–æ–≥–æ –Ω–∞–±–æ—Ä–∞ (–º–æ–∂–Ω–æ —Å–ª—É—á–∞–π–Ω—ã–π)
    i_train = np.random.randint(len(train_X))
    i_val = np.random.randint(len(val_X))
    i_test = np.random.randint(len(test_X))

    plot_raw_curve(train_X_raw, train_Y, i_train, "Train")
    plot_raw_curve(val_X_raw, val_Y, i_val, "Val")
    plot_raw_curve(test_X_raw, test_Y, i_test, "Test")

    train_dataset = MagnetizationDataset(train_X, train_Y)
    val_dataset = MagnetizationDataset(val_X, val_Y)
    test_dataset = MagnetizationDataset(test_X, test_Y)

    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE_PARAMETER, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE_PARAMETER, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE_PARAMETER, shuffle=False)

    model = Net()
    # –∑–∞–¥–∞—ë–º —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å (loss-—Ñ—É–Ω–∫—Ü–∏—é) –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏.
    # SmoothL1Loss (—Ç–∞–∫–∂–µ –∏–∑–≤–µ—Å—Ç–Ω–∞ –∫–∞–∫ Huber-loss)
    # —Å–æ—á–µ—Ç–∞–µ—Ç –≤ —Å–µ–±–µ
    # ¬´L1-—à—É–º–æ—É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å¬ª (–¥–ª—è –∫—Ä—É–ø–Ω—ã—Ö –æ—à–∏–±–æ–∫) –∏
    # ¬´L2-–ø–ª–∞–≤–Ω–æ—Å—Ç—å¬ª (–¥–ª—è –º–µ–ª–∫–∏—Ö),
    # —á—Ç–æ —á–∞—Å—Ç–æ –¥–∞—ë—Ç –±–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –≤—ã–±—Ä–æ—Å–∞—Ö, —á–µ–º –ø—Ä–æ—Å—Ç–æ MSE –∏–ª–∏ MAE.
    criterion = nn.SmoothL1Loss()

    # –≤—ã–±–∏—Ä–∞–µ–º –∞–ª–≥–æ—Ä–∏—Ç–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ Adam, –∫–æ—Ç–æ—Ä—ã–π –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç —Ç–µ–º–ø –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞.
    # weight_decay ‚Äî L2-—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è (—à—Ç—Ä–∞—Ñ –∑–∞ –±–æ–ª—å—à–∏–µ –≤–µ—Å–∞), –ø–æ–º–æ–≥–∞–µ—Ç –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—Ç–∏—Ç—å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ.
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE_PARAMETER, weight_decay=WEIGHT_DECAY_PARAMETER)

    # —Å–æ–∑–¥–∞—ë–º ¬´—à–µ–¥—É–ª–µ—Ä¬ª —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è:
    # –∫–∞–∂–¥—ã–µ step_size=50 —ç–ø–æ—Ö —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è –±—É–¥–µ—Ç —É–º–Ω–æ–∂–∞—Ç—å—Å—è –Ω–∞ gamma=0.5 (—Ç. –µ. —Å–Ω–∏–∂–∞—Ç—å—Å—è –≤–¥–≤–æ–µ).
    # –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –Ω–∞ —Å—Ç–∞—Ä—Ç–µ –±—ã—Å—Ç—Ä–æ —Å—Ö–æ–¥–∏—Ç—å—Å—è, –∞ –ø–æ –º–µ—Ä–µ –ø—Ä–∏–±–ª–∏–∂–µ–Ω–∏—è –∫ –º–∏–Ω–∏–º—É–º—É –¥–µ–ª–∞—Ç—å –º–µ–Ω—å—à–∏–µ —à–∞–≥–∏ –∏ —Ç–æ—á–Ω–µ–µ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å –≤–µ—Å–∞.
    scheduler = StepLR(optimizer, step_size=50, gamma=0.5)

    train_losses, val_losses = [], []

    logging.info("Start training...")
    for epoch in range(num_epochs):

        '''
        –ò—Ç–∞–∫, –∫–∞–∂–¥–∞—è —ç–ø–æ—Ö–∞ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑:
            1. –¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ –Ω–∞ –≤—Å–µ—Ö –±–∞—Ç—á–∞—Ö (—Å –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ–º –≤–µ—Å–æ–≤).
            2. –í–∞–ª–∏–¥–∞—Ü–∏—è (–±–µ–∑ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –≤–µ—Å–æ–≤).
            3. –ö–æ—Ä—Ä–µ–∫—Ü–∏—è —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—É—â–µ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏.
        '''

        # ------------ 1. –¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–π —à–∞–≥
        model.train()
        running_loss = 0
        for batch_X, batch_Y in train_loader:
            # batch_X ‚Äî –±–∞—Ç—á –≤—Ö–æ–¥–Ω—ã—Ö –∫—Ä–∏–≤—ã—Ö, batch_Y ‚Äî –±–∞—Ç—á –∏—Ö —Ü–µ–ª–µ–≤—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.

            # –æ–±–Ω—É–ª—è–µ–º –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –≤–æ –≤—Å–µ—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–∞—Ö (–∏–Ω–∞—á–µ –æ–Ω–∏ –±—ã —Å—É–º–º–∏—Ä–æ–≤–∞–ª–∏—Å—å).
            optimizer.zero_grad()

            # –ø—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ (forward): –ø–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–µ—Ç–∏ –¥–ª—è –±–∞—Ç—á–∞.
            outputs = model(batch_X)

            # –≤—ã—á–∏—Å–ª—è–µ–º —Ç–µ–∫—É—â—É—é –æ—à–∏–±–∫—É (Smooth L1) –º–µ–∂–¥—É –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏ –∏ –∏—Å—Ç–∏–Ω–Ω—ã–º–∏ J.
            loss = criterion(outputs, batch_Y)

            # –æ–±—Ä–∞—Ç–Ω—ã–π –ø—Ä–æ—Ö–æ–¥: PyTorch –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã—á–∏—Å–ª—è–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –≤—Å–µ—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å–µ—Ç–∏.
            loss.backward()

            # –æ–±–Ω–æ–≤–ª—è–µ–º –≤–µ—Å–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –∏ —Ç–µ–∫—É—â–µ–≥–æ learning rate.
            optimizer.step()

            # –Ω–∞–∫–∞–ø–ª–∏–≤–∞–µ–º —Å—É–º–º—É loss‚Äô–æ–≤, —É–º–Ω–æ–∂–µ–Ω–Ω—ã—Ö –Ω–∞ —á–∏—Å–ª–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤ –±–∞—Ç—á–µ (—á—Ç–æ–±—ã –ø–æ—Ç–æ–º —É—Å—Ä–µ–¥–Ω–∏—Ç—å –ø–æ –≤—Å–µ–º –æ–±—Ä–∞–∑—Ü–∞–º).
            running_loss += loss.item() * batch_X.size(0)

        # –ø–æ—Å–ª–µ –≤—Å–µ—Ö –±–∞—Ç—á–µ–π –¥–µ–ª–∏–º –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—É—é —Å—É–º–º—É –Ω–∞ –æ–±—â–µ–µ —á–∏—Å–ª–æ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤,
        # –ø–æ–ª—É—á–∞—è —Å—Ä–µ–¥–Ω—é—é –æ—à–∏–±–∫—É –∑–∞ —ç–ø–æ—Ö—É, –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –µ—ë –≤ —Å–ø–∏—Å–æ–∫ train_losses.
        epoch_loss = running_loss / len(train_loader.dataset)
        train_losses.append(epoch_loss)

        # ------------ 2. –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π —à–∞–≥

        # –ø–µ—Ä–µ–≤–æ–¥–∏–º –º–æ–¥–µ–ª—å –≤ ¬´—Ä–µ–∂–∏–º –æ—Ü–µ–Ω–∫–∏¬ª: Dropout –æ—Ç–∫–ª—é—á–∞–µ—Ç—Å—è, BatchNorm –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏, –∞ –Ω–µ —Ç–µ–∫—É—â–∏–π –±–∞—Ç—á.
        model.eval()
        running_val_loss = 0

        # –û–±–æ—Ä–∞—á–∏–≤–∞–µ–º –≤ torch.no_grad(), —á—Ç–æ–±—ã –Ω–µ –Ω–∞–∫–∞–ø–ª–∏–≤–∞—Ç—å –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –∏ –Ω–µ —Ç—Ä–∞—Ç–∏—Ç—å –ø–∞–º—è—Ç—å
        with torch.no_grad():
            for batch_X, batch_Y in val_loader:
                outputs = model(batch_X)
                loss = criterion(outputs, batch_Y)
                running_val_loss += loss.item() * batch_X.size(0)

        # —É—Å—Ä–µ–¥–Ω—è–µ–º –ø–æ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–º—É –Ω–∞–±–æ—Ä—É –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ val_losses
        epoch_val_loss = running_val_loss / len(val_loader.dataset)
        val_losses.append(epoch_val_loss)

        # ------------ 3. –ö–æ—Ä—Ä–µ–∫—Ü–∏—è —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è
        # –æ–±–Ω–æ–≤–ª—è–µ–º learning rate —Å–æ–≥–ª–∞—Å–Ω–æ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—é (StepLR —É–º–µ–Ω—å—à–∏—Ç –µ–≥–æ –≤–¥–≤–æ–µ –∫–∞–∂–¥—ã–µ 50 —ç–ø–æ—Ö).
        scheduler.step()

        # ------------ 4. –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        # –≤—ã–≤–æ–¥–∏–º –≤ –∫–æ–Ω—Å–æ–ª—å –ø—Ä–æ–≥—Ä–µ—Å—Å: –Ω–æ–º–µ—Ä —Ç–µ–∫—É—â–µ–π —ç–ø–æ—Ö–∏ –∏ —Å—Ä–µ–¥–Ω–∏–µ –æ—à–∏–±–∫–∏ –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–µ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏, –æ–∫—Ä—É–≥–ª—ë–Ω–Ω—ã–µ –¥–æ 6 –∑–Ω–∞–∫–æ–≤.
        logging.info(f"Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_loss:.6f}, Val Loss: {epoch_val_loss:.6f}")

    torch.save(model.state_dict(), TRAINED_MODEL_FILE)
    logging.info(f"Model saved to {TRAINED_MODEL_FILE}")

    # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∫—Ä–∏–≤–æ–π –æ–±—É—á–µ–Ω–∏—è
    plt.figure()
    plt.plot(train_losses, label='Training Loss')
    plt.plot(val_losses, label='Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)
    plt.savefig(LEARNING_CURVE_PLOT_FILE)
    plt.show()

if __name__ == "__main__":
    main()
